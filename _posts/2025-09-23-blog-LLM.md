---
title: 'Collection of Information on Large Language Models'
date: 2025-09-23
permalink: /posts/blog/llm
---

To prepare for an MLsys-related internship, I have organized some relevant resources. I'm sharing them here in the hope that they may help others as well.


<!-- excerpt -->

## Transformer
#### Transformer介绍 
Transformer 诞生于 2017 年，由 Vaswani 等人在论文《Attention Is All You Need》中首次提出。和传统网络RNN，CNN相比，Transformer具有极大的优势。Transformer 采用完全基于注意力机制（Attention）的架构，通过自注意力（Self-Attention）机制能够直接捕捉序列中任意位置的依赖。借助多头注意力（Multi-Head Attention）和位置编码（Positional Encoding）Transformer迅速取代 RNN 和 CNN，成为现代 NLP 的核心基础模型。


#### Model Architecture （编码器和解码器）
![模型架构](https://raw.githubusercontent.com/BV003/images/main/llm/0.png)
![模型架构](https://raw.githubusercontent.com/BV003/images/main/llm/1.png)

编码器由𝑁=6层相同的层堆叠而成。每一层包含两个子层。第一个子层是多头自注意力机制（multi-head self-attention），第二个子层是简单的逐点全连接前馈网络（position-wise fully connected feed-forward network）。我们在每个子层周围使用残差连接（residual connection），随后进行层归一化（layer normalization）。也就是说，每个子层的输出为

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

解码器（Decoder）：解码器同样由 N=6 层相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器还插入了第三个子层，对编码器堆栈的输出进行多头注意力计算（multi-head attention）。与编码器类似，我们在每个子层周围使用残差连接，并进行层归一化。

残差连接就是把每个子层的输入直接加到子层输出上，再经过归一化，这样既保留了原始信息，又缓解了深层网络的梯度消失问题，使训练更稳定、收敛更快。

我们还修改了解码器堆栈中的自注意力子层，使得每个位置无法关注后续位置。这种掩码（masking）结合输出嵌入向量偏移一个位置的方式，确保位置 \\( i \\) 的预测只能依赖于位置小于 \\( i \\) 的已知输出。

#### Attention 机制
注意力（attention）函数可以被描述为：将一个查询（query）和一组键值对（key-value pairs）映射为一个输出，其中查询、键、值以及输出都是向量。输出通过对值向量的加权求和得到，而每个值的权重由查询与对应键之间的兼容性函数（compatibility function）计算得出。

我们可以定义缩放点积注意力（Scaled Dot-Product Attention）。输入包括维度为 \\( d_k \\) 的查询（queries）和键（keys），以及维度为 \\( d_v \\) 的值（values）。我们先计算查询与所有键的点积，然后除以 \\(  \sqrt{d_k} \\)，再通过 softmax 函数(激活函数，让模型可以学习非线性的真实世界)得到各值向量的权重（Softmax 是一种常用的归一化函数）。


在实际操作中，我们通常同时对一组查询计算注意力函数，并将其打包成矩阵 
\\(Q\\)。键和值也分别打包成矩阵 \\(K\\) 和 \\(V\\)。输出矩阵的计算公式为：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

我们引入通过 \\(  \sqrt{d_k} \\) 的目的是防止点积过大导致 softmax 的梯度变得非常小，保证训练稳定性。

```
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    """
    实现 Scaled Dot-Product Attention
    公式: Attention(Q, K, V) = softmax((Q K^T)/√d_k) V
    """

    def __init__(self, input_dim, dim_k, dim_v):
        """
        Args:
            input_dim (int): 输入特征维度（d_model）
            dim_k (int): 查询和键的维度
            dim_v (int): 值的维度
        """
        super().__init__()
        
        # 线性映射，将输入映射到 Q, K, V
        self.q = nn.Linear(input_dim, dim_k)  # 查询
        self.k = nn.Linear(input_dim, dim_k)  # 键
        self.v = nn.Linear(input_dim, dim_v)  # 值
        
        # 缩放因子 √d_k
        self.scale = math.sqrt(dim_k)

    def forward(self, x):
        """
        Args:
            x: 输入张量, 维度 (batch_size, seq_len, input_dim)
        Returns:
            output: 注意力输出, 维度 (batch_size, seq_len, dim_v)
        """
        # 1. 线性映射到 Q, K, V
        # 输出维度: (batch_size, seq_len, dim_k) 或 (batch_size, seq_len, dim_v)
        Q = self.q(x)
        K = self.k(x)
        V = self.v(x)

        # 2. 计算注意力分数
        # 使用 matmul 可以直接对 3D 张量做 batch 矩阵乘法
        # K.transpose(-2, -1) 将 (batch, seq_len, dim_k) -> (batch, dim_k, seq_len)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (batch, seq_len, seq_len)

        # 3. 对注意力分数做 softmax，得到注意力权重
        attn_weights = torch.softmax(scores, dim=-1)  # 每行归一化

        # 4. 注意力权重乘以 V
        output = torch.matmul(attn_weights, V)  # (batch, seq_len, dim_v)

        return output
```


#### Multi-Head Attention (MHA)

![多头注意力](https://raw.githubusercontent.com/BV003/images/main/llm/2.png)

我们发现，将查询、键和值分别通过 h 次不同的、可学习的线性映射投影到 \\( d_k \\)、\\( d_k \\) 和 \\( d_v \\) 维度上更为有效。在这些投影后的查询、键和值上，我们并行执行注意力函数，得到 \\( d_v \\) 维度的输出值。

多头注意力允许模型同时关注来自不同表示子空间的不同位置的信息。而单头注意力在平均操作下会抑制这一能力。多头注意力让模型并行学习多种关系，不同 head 可以学到不同的关注模式。

公式表示为：

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

其中：

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

投影矩阵为参数矩阵：

$$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$$

$$W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$$

$$W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$$

$$W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}}$$


在本工作中，我们使用 h = 8 个并行注意力层（即头）。对于每个头，我们设置 \\( d_k \\) = \\( d_v \\) =\\( d_{\text{model}} \\) /h = 64。由于每个头的维度较小，总计算成本与使用全维度的单头注意力相当。

```
import torch
import torch.nn as nn
import math

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, input_dim, num_heads, dim_k, dim_v):
        """
        Multi-Head Self-Attention
        
        Args:
            input_dim (int): 输入特征维度（d_model）
            num_heads (int): 注意力头数量（h）
            dim_k (int): 所有头的总查询/键维度（通常 = input_dim）
            dim_v (int): 所有头的总值维度（通常 = input_dim）
        """
        super().__init__()
        assert dim_k % num_heads == 0, "dim_k must be divisible by num_heads"
        assert dim_v % num_heads == 0, "dim_v must be divisible by num_heads"

        self.num_heads = num_heads
        self.dim_per_head_k = dim_k // num_heads
        self.dim_per_head_v = dim_v // num_heads

        # Q、K、V 的线性映射层
        self.q_linear = nn.Linear(input_dim, dim_k)
        self.k_linear = nn.Linear(input_dim, dim_k)
        self.v_linear = nn.Linear(input_dim, dim_v)

        # 输出线性映射，把拼接后的多头输出映射回 input_dim
        self.out_linear = nn.Linear(dim_v, input_dim)

        # 缩放因子 √d_k
        self.scale = math.sqrt(self.dim_per_head_k)

    def forward(self, x):
        """
        x: (batch_size, seq_len, input_dim)
        """
        b, s, _ = x.size()

        # 1. 线性映射到 Q, K, V
        # 输出维度: (b, s, num_heads * dim_per_head)
        Q = self.q_linear(x)
        K = self.k_linear(x)
        V = self.v_linear(x)

        # 2. reshape 并转置为多头: (b, num_heads, s, dim_per_head)
        # view拆分最后一位
        Q = Q.view(b, s, self.num_heads, self.dim_per_head_k).transpose(1, 2)
        K = K.view(b, s, self.num_heads, self.dim_per_head_k).transpose(1, 2)
        V = V.view(b, s, self.num_heads, self.dim_per_head_v).transpose(1, 2)

        # 3. 计算注意力分数
        # Q * K^T / sqrt(d_k)
        # Q: (b, h, s, d_k), K: (b, h, s, d_k) -> (b, h, s, s)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale

        # 4. softmax 得到注意力权重
        attn_weights = torch.softmax(scores, dim=-1)

        # 5. 权重乘以 V 得到每个头的输出: (b, h, s, dim_per_head_v)
        attn_output = torch.matmul(attn_weights, V)

        # 6. 转置并拼接多头输出: (b, s, num_heads * dim_per_head_v)
        attn_output = attn_output.transpose(1, 2).contiguous().view(b, s, -1)

        # 7. 输出线性映射回 input_dim: (b, s, input_dim)
        out = self.out_linear(attn_output)

        return out

```
Masked 版本在 softmax 前增加一个 mask：

$$
\text{MaskedAttention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} + M \right) V
$$

其中 $$M \in \mathbb{R}^{\text{seq_len} \times \text{seq_len}}$$ 是上三角掩码矩阵

掩码元素：

$$
M_{i,j} =
\begin{cases}
0, & j \leq i \\
-\infty, & j > i
\end{cases}
$$

这样 softmax 后，未来 token 的注意力权重被 置为 0，模型无法访问未来信息。

#### Feed-Forward Network, FFN
FFN是MLP多层感知机的一种。  
除了注意力子层之外，我们的编码器和解码器中的每一层还包含一个 全连接前馈网络，它会分别并且一致地应用于序列中的每一个位置。  
这个网络由两个线性变换和中间的一个 ReLU 激活函数组成：

$$
\text{FFN}(x) = \max\left(0, xW_1 + b_1\right)W_2 + b_2
$$

尽管这些线性变换在不同位置上是相同的，但在不同层之间它们的参数是不同的。
输入和输出的维度为\\( d_{\text{model}} \\)=512，而内部层的维度为\\( d_{\text{ff}} \\)=2048。

```
import torch
import torch.nn as nn

class FFN(nn.Module):
    def __init__(self, d_model=512, d_ff=2048, dropout=0.0):
        super().__init__()
        # 等价于两个线性层和中间的 ReLU
        self.w1 = nn.Linear(d_model, d_ff)   # x @ W1^T + b1  （PyTorch Linear 自带偏置）
        self.w2 = nn.Linear(d_ff, d_model)   # hidden @ W2^T + b2
        self.relu = nn.ReLU()
        # dropout为丢弃神经元的概率
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        hidden = self.w1(x)       # (batch, seq, d_ff)
        hidden = self.relu(hidden)
        hidden = self.dropout(hidden)
        out = self.w2(hidden)     # (batch, seq_len, d_model)
        return out
```

#### Embeddings and Softmax
我们使用 可学习的嵌入（learned embeddings） 将输入和输出的 token 转换为维度为 \\( d_{\text{model}} \\)的向量。  
同时，我们也使用通常的可学习线性变换（linear transformation）和softmax函数，将解码器的输出转换为预测的下一个 token的概率分布。  
在我们的模型中，两个嵌入层（输入嵌入和输出嵌入）以及 softmax 前的线性变换共享相同的权重矩阵。
在嵌入层中，我们还会将这些权重乘以 \\( \sqrt{d_{\text{model}}} \\)进行缩放。

#### Positional Encoding
我们在编码器和解码器堆栈底部的输入嵌入中，加入了位置编码（positional encodings）。位置编码的维度与嵌入维度相同\\( d_{\text{model}} \\)，以便可以直接与嵌入向量相加。

$$
\begin{align*}
\text{PE}(pos, 2i) &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
\text{PE}(pos, 2i + 1) &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{align*}
$$

其中 \\( pos\\)表示位置，\\(i\\)表示维度索引。换句话说，每一个维度对应一个正弦或余弦函数。

#### Layer Normalization
归一化的目的主要是加速训练、稳定梯度、改善收敛性，缓解超参数敏感性。LayerNorm 通常放在残差连接之后，对某层子模块输出\\(x\\)，残差连接为：

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

\\( Sublayer\\)可以是 Self-Attention 或 FFN。






## 大语言模型原理概述

Decoder-Only 就是目前大火的 LLM 的基础架构，目前所有的 LLM 基本都是 Decoder-Only 模型（RWKV、Mamba 等非 Transformer 架构除外）。而引发 LLM 热潮的 ChatGPT，正是 Decoder-Only 系列的代表模型 GPT 系列模型的大成之作。而目前作为开源 LLM 基本架构的 LLaMA 模型，也正是在 GPT 的模型架构基础上优化发展而来。

### GPT
GPT，即 Generative Pre-Training Language Model，是由 OpenAI 团队于 2018年发布的预训练语言模型。

#### 模型架构——Decoder Only
![gpt架构](https://raw.githubusercontent.com/BV003/images/main/llm/3.png)

对于一个自然语言文本的输入，先通过 tokenizer 进行分词并转化为对应词典序号的 input_ids。
输入的 input_ids 首先通过 Embedding 层，再经过 Positional Embedding 进行位置编码。不同于 BERT 选择了可训练的全连接层作为位置编码，GPT 沿用了 Transformer 的经典 Sinusoidal 位置编码，即通过三角函数进行绝对位置编码。

通过 Embedding 层和 Positional Embedding 层编码成 hidden_states 之后，就可以进入到解码器（Decoder），第一代 GPT 模型和原始 Transformer 模型类似，选择了 12层解码器层，但是在解码器层的内部，相较于 Transformer 原始 Decoder 层的双注意力层设计，GPT 的 Decoder 层反而更像 Encoder 层一点。由于不再有 Encoder 的编码输入，Decoder 层仅保留了一个带掩码的注意力层，并且将 LayerNorm 层从 Transformer 的注意力层之后提到了注意力层之前。hidden_states 输入 Decoder 层之后，会先进行 LayerNorm，再进行掩码注意力计算，然后经过残差连接和再一次 LayerNorm 进入到 MLP 中并得到最后输出。

由于不存在 Encoder 的编码结果，Decoder 层中的掩码注意力也是自注意力计算。也就是对一个输入的 hidden_states，会通过三个参数矩阵来生成 query、key 和 value，而不再是像 Transformer 中的 Decoder 那样由 Encoder 输出作为 key 和 value。

#### 预训练任务——CLM

Decoder-Only 的模型结构往往更适合于文本生成任务，因此，Decoder-Only 模型往往选择了最传统也最直接的预训练任务——因果语言模型，Casual Language Model，下简称 CLM。

CLM 可以看作 N-gram 语言模型的一个直接扩展。N-gram 语言模型是基于前 N 个 token 来预测下一个 token，CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token，通过不断重复该过程来实现目标文本序列的生成。也就是说，CLM 是一个经典的补全形式。例如，CLM 的输入和输出可以是：
```
input: 今天天气
output: 今天天气很

input: 今天天气很
output：今天天气很好

```
因此，对于一个输入目标序列长度为 256，期待输出序列长度为 256 的任务，模型会不断根据前 256 个 token、257个 token（输入+预测出来的第一个 token）...... 进行 256 次计算，最后生成一个序列长度为 512 的输出文本，这个输出文本前 256 个 token 为输入，后 256 个 token 就是我们期待的模型输出。CLM 也可以使用海量的自然语言语料进行大规模的预训练。

#### GPT 系列模型的发展
自 GPT-1 推出开始，OpenAI 一直坚信 Decoder-Only 的模型结构和“体量即正义”的优化思路，不断扩大预训练数据集、模型体量并对模型做出一些小的优化和修正，来不断探索更强大的预训练模型。从被 BERT 压制的 GPT-1，到没有引起足够关注的 GPT-2，再到激发了涌现能力、带来大模型时代的 GPT-3，最后带来了跨时代的 ChatGPT，OpenAI 通过数十年的努力证明了其思路的正确性。

下表总结了从 GPT-1 到 GPT-3 的模型结构、预训练语料大小的变化：

| 模型   | Decoder Layer | Hidden_size | 注意力头数 | 注意力维度 | 总参数量 | 预训练语料 |
| ------ | ------------- | ----------- | ---------- | ---------- | -------- | ---------- |
| GPT-1  | 12            | 3072        | 12         | 768        | 0.12B    | 5GB        |
| GPT-2  | 48            | 6400        | 25         | 1600       | 1.5B     | 40GB       |
| GPT-3  | 96            | 49152       | 96         | 12288      | 175B     | 570GB      |


### LLAVA
![llava架构](https://raw.githubusercontent.com/BV003/images/main/llm/9.png)

LLaVA的网络架构如上图所示，主要有几部分组成:
- Language Model: LLaMa，生成word embedding
- Vision Encoder: CLIP的视觉编码器ViT
- 视觉-语言对齐模块（Projection Layer / Connector），一个小型 MLP 或线性层，用于将视觉特征映射到语言模型的词向量空间。作用是“连接”视觉特征与语言输入，使 LLM 能够直接处理图像信息。

LLaVA 采用两阶段微调策略：阶段一：冻结 LLM 权重和视觉编码器，只微调线性层，目的是对齐视觉特征 
和语言特征。阶段二：只冻结视觉编码器，微调 LLM 权重和线性层。

## LLM的训练流程
![训练流程](https://raw.githubusercontent.com/BV003/images/main/llm/4.jpg)

一般而言，训练一个完整的 LLM 需要经过图中的三个阶段——Pretrain、SFT 和 RLHF。

### 预训练
Pretrain，即预训练，是训练 LLM 最核心也是工程量最大的第一步。LLM 的预训练和传统预训练模型非常类似，同样是使用海量无监督文本对随机初始化的模型参数进行训练。预训练任务也都沿承了 GPT 模型的经典预训练任务——因果语言模型（Causal Language Model，CLM）。因果语言模型建模，即和最初的语言模型一致，通过给出上文要求模型预测下一个 token 来进行训练。LLM被喂进大量的文本，比如书、网页、对话、代码等，每次训练时，它看到一段话的前半部分，要去猜接下来的词。标准答案即为原始的语料，训练目标是最大化训练语料的似然，可以通过一系列的损失函数实现。

根据定义，LLM 的核心特点即在于其具有远超传统预训练模型的参数量，同时在更海量的语料上进行预训练。传统预训练模型如 BERT，有 base 和 large 两个版本。BERT-base 模型由 12个 Encoder 层组成，其 hidden_size 为 768，使用 12个头作为多头注意力层，整体参数量为 1亿（110M）；而 BERT-large 模型由 24个 Encoder 层组成，hidden_size 为 1024，有 16个头，整体参数量为 3亿（340M）。同时，BERT 预训练使用了 33亿（3B）token 的语料，在 64块 TPU 上训练了 4天。事实上，相对于传统的深度学习模型，3亿参数量、33亿训练数据的 BERT 已经是一个能力超群、资源消耗巨大的庞然大物。

但是，一般而言的 LLM 通常具有数百亿甚至上千亿参数，即使是广义上最小的 LLM，一般也有十亿（1B）以上的参数量。例如以开山之作 GPT-3 为例，其有 96个 Decoder 层，12288 的 hidden_size 和 96个头，共有 1750亿（175B）参数，比 BERT 大出快 3个数量级。即使是目前流行的小型 LLM 如 Qwen-1.8B，其也有 24个 Decoder 层、2048的 hidden_size 和 16个注意力头，整体参数量达到 18亿（1.8B）。

| 模型        | hidden_layers（隐藏层数） | hidden_size（隐藏层维度） | heads（注意力头数） | 整体参数量 | 预训练数据量 |
|-------------|---------------------------|---------------------------|---------------------|------------|--------------|
| BERT-base   | 12                        | 768                       | 12                  | 0.1B       | 3B           |
| BERT-large  | 24                        | 1024                      | 16                  | 0.3B       | 3B           |
| Qwen-1.8B   | 24                        | 2048                      | 16                  | 1.8B       | 2.2T         |
| LLaMA-7B    | 32                        | 4096                      | 32                  | 7B         | 1T           |
| GPT-3       | 96                        | 12288                     | 96                  | 175B       | 300B         |

更重要的是，LLM 往往需要使用更大规模的预训练语料。根据由 OpenAI 提出的 Scaling Law：C ~ 6ND，其中 C 为计算量，N 为模型参数，D 为训练的 token 数，可以实验得出训练 token 数应该是模型参数的 1.7倍，也就是说 175B 的 GPT-3，需要使用 300B token 进行预训练。而 LLaMA 更是进一步提出，使用 20倍 token 来训练模型能达到效果最优，因此 175B 的 GPT-3，可以使用3.5T token 数据预训练达到最优性能。

如此庞大的模型参数和预训练数据，使得预训练一个 LLM 所需要的算力资源极其庞大。事实上，哪怕是预训练一个 1B 的大模型，也至少需要多卡分布式 GPU 集群，通过分布式框架对模型参数、训练的中间参数和训练数据进行切分，才能通过以天为单位的长时间训练来完成。一般来说，百亿级 LLM 需要 1024张 A100 训练一个多月，而十亿级 LLM 一般也需要 256张 A100 训练两、三天，计算资源消耗非常高。

也正因如此，分布式训练框架也成为 LLM 训练必不可少的组成部分。分布式训练框架的核心思路是数据并行和模型并行。所谓数据并行，是指训练模型的尺寸可以被单个 GPU 内存容纳，但是由于增大训练的 batch_size 会增大显存开销，无法使用较大的 batch_size 进行训练；同时，训练数据量非常大，使用单张 GPU 训练时长难以接受。

因此，如下图所示可以让模型实例在不同 GPU 和不同批数据上运行，每一次前向传递完成之后，收集所有实例的梯度并计算梯度更新，更新模型参数之后再传递到所有实例。也就是在数据并行的情况下，每张 GPU 上的模型参数是保持一致的，训练的总批次大小等于每张卡上的批次大小之和。

![数据并行](https://raw.githubusercontent.com/BV003/images/main/llm/5.jpg)

但是，当 LLM 扩大到上百亿参数，单张 GPU 内存往往就无法存放完整的模型参数。如下图所示，在这种情况下，可以将模型拆分到多个 GPU 上，每个 GPU 上存放不同的层或不同的部分，从而实现模型并行。

![模型和数据并行](https://raw.githubusercontent.com/BV003/images/main/llm/6.jpg)

关于并行更详细的内容放在后面主流的分布式训练框架Deepspeed讲。

除去计算资源的要求，训练数据本身也是预训练 LLM 的一个重大挑战。训练一个 LLM，至少需要数百 B 甚至上 T 的预训练语料。根据研究，LLM 所掌握的知识绝大部分都是在预训练过程中学会的，因此，为了使训练出的 LLM 能够覆盖尽可能广的知识面，预训练语料需要组织多种来源的数据，并以一定比例进行混合。目前，主要的开源预训练语料包括 CommonCrawl、C4、Github、Wikipedia 等。不同的 LLM 往往会在开源预训练语料基础上，加入部分私有高质量语料，再基于自己实验得到的最佳配比来构造预训练数据集。事实上，数据配比向来是预训练 LLM 的“核心秘籍”，不同的配比往往会相当大程度影响最终模型训练出来的性能。例如，下表展示了 LLaMA 的预训练数据及配比：

数据集|占比|数据集大小（Disk size）
-----|----|---------------------
CommonCrawl|67.0%|3.3 TB
C4|15.0%|783 GB
Github|4.5%|328 GB
Wikipedia|4.5%|83 GB
Books|4.5%|85 GB
ArXiv|2.5%|92 GB
StackExchange|2.0%|78 GB

训练一个中文 LLM，训练数据的难度会更大。目前，高质量语料还是大部分集中在英文范畴，例如上表的 Wikipedia、Arxiv 等，均是英文数据集；而 C4 等多语言数据集中，英文语料也占据主要地位。目前开源的中文 LLM 如 ChatGLM、Baichuan 等模型均未开放其预训练数据集，开源的中文预训练数据集目前仅有昆仑天工开源的[SkyPile](https://huggingface.co/datasets/Skywork/SkyPile-150B)（150B）、中科闻歌开源的[yayi2](https://huggingface.co/datasets/wenge-research/yayi2_pretrain_data)（100B）等，相较于英文开源数据集有明显差距。

预训练数据的处理与清洗也是 LLM 预训练的一个重要环节。诸多研究证明，预训练数据的质量往往比体量更加重要。预训练数据处理一般包括以下流程：

1. 文档准备。由于海量预训练语料往往是从互联网上获得，一般需要从爬取的网站来获得自然语言文档。文档准备主要包括 URL 过滤（根据网页 URL 过滤掉有害内容）、文档提取（从 HTML 中提取纯文本）、语言选择（确定提取的文本的语种）等。
2. 语料过滤。语料过滤的核心目的是去除低质量、无意义、有毒有害的内容，例如乱码、广告等。语料过滤一般有两种方法：基于模型的方法，即通过高质量语料库训练一个文本分类器进行过滤；基于启发式的方法，一般通过人工定义 web 内容的质量指标，计算语料的指标值来进行过滤。
3. 语料去重。实验表示，大量重复文本会显著影响模型的泛化能力，因此，语料去重即删除训练语料中相似度非常高的文档，也是必不可少的一个步骤。去重一般基于 hash 算法计算数据集内部或跨数据集的文档相似性，将相似性大于指定阈值的文档去除；也可以基于子串在序列级进行精确匹配去重。

### SFT

预训练是 LLM 强大能力的根本来源，事实上，LLM 所覆盖的海量知识基本都是源于预训练语料。LLM 的性能本身，核心也在于预训练的工作。但是，预训练赋予了 LLM 能力，却还需要第二步将其激发出来。经过预训练的 LLM 好像一个博览群书但又不求甚解的书生，对什么样的偏怪问题，都可以流畅地接出下文，但他偏偏又不知道问题本身的含义，只会“死板背书”。这一现象的本质是因为，LLM 的预训练任务就是经典的 CLM，也就是训练其预测下一个 token 的能力，在没有进一步微调之前，其无法与其他下游任务或是用户指令适配。

因此，我们还需要第二步来教这个博览群书的学生如何去使用它的知识，也就是 SFT（Supervised Fine-Tuning，有监督微调）。面对能力强大的 LLM，我们往往不再是在指定下游任务上构造有监督数据进行微调，而是选择训练模型的“通用指令遵循能力”，也就是一般通过指令微调的方式来进行 SFT。所谓指令微调，即我们训练的输入是各种类型的用户指令，而需要模型拟合的输出则是我们希望模型在收到该指令后做出的回复。例如，我们的一条训练样本可以是：

```
input:告诉我今天的天气预报？
output:根据天气预报，今天天气是晴转多云，最高温度26摄氏度，最低温度9摄氏度，昼夜温差大，请注意保暖哦
```

也就是说，SFT 的主要目标是让模型从多种类型、多种风格的指令中获得泛化的指令遵循能力，也就是能够理解并回复用户的指令。因此，类似于 Pretrain，SFT 的数据质量和数据配比也是决定模型指令遵循能力的重要因素。

首先是指令数据量及覆盖范围。为了使 LLM 能够获得泛化的指令遵循能力，即能够在未训练的指令上表现良好，需要收集大量类别各异的用户指令和对应回复对 LLM 进行训练。一般来说，在单个任务上 500~1000 的训练样本就可以获得不错的微调效果。但是，为了让 LLM 获得泛化的指令遵循能力，在多种任务指令上表现良好，需要在训练数据集中覆盖多种类型的任务指令，同时也需要相对较大的训练数据量，表现良好的开源 LLM SFT 数据量一般在数 B token 左右。

为提高 LLM 的泛化能力，指令数据集的覆盖范围自然是越大越好。但是，多种不同类型的指令数据之间的配比也是 LLM 训练的一大挑战。OpenAI 训练的 InstructGPT（即 ChatGPT 前身）使用了源自于用户使用其 API 的十种指令：

指令类型|占比
-------|-----
文本生成|45.6%
开放域问答|12.4%
头脑风暴|11.2%
聊天|8.4%
文本转写|6.6%
文本总结|4.2%
文本分类|3.5%
其他|3.5%
特定域问答|2.6%
文本抽取|1.9%

高质量的指令数据集具有较高的获取难度。不同于预训练使用的无监督语料，SFT 使用的指令数据集是有监督语料，除去设计广泛、合理的指令外，还需要对指令回复进行人工标注，并保证标注的高质量。事实上，ChatGPT 的成功很大一部分来源于其高质量的人工标注数据。但是，人工标注数据成本极高，也罕有企业将人工标注的指令数据集开源。为降低数据成本，部分学者提出了使用 ChatGPT 或 GPT-4 来生成指令数据集的方法。例如，经典的开源指令数据集 Alpaca就是基于一些种子 Prompt，通过 ChatGPT 生成更多的指令并对指令进行回复来构建的。

一般 SFT 所使用的指令数据集包括以下三个键：

```json
{
    "instruction":"即输入的用户指令",
    "input":"执行该指令可能需要的补充输入，没有则置空",
    "output":"即模型应该给出的回复"
}
```

例如，如果我们的指令是将目标文本“今天天气真好”翻译成英文，那么该条样本可以构建成如下形式：

```json
{
    "instruction":"将下列文本翻译成英文：",
    "input":"今天天气真好",
    "output":"Today is a nice day！"
}
```

同时，为使模型能够学习到和预训练不同的范式，在 SFT 的过程中，往往会针对性设置特定格式。例如，LLaMA 的 SFT 格式为：

```
    ### Instruction:\n content \n\n### Response:\n
```

其中的 content 即为具体的用户指令，也就是说，对于每一个用户指令，将会嵌入到上文的 content 部分，这里的用户指令不仅指上例中的 “instruction”，而是指令和输入的拼接，即模型可以执行的一条完整指令。例如，针对上例，LLaMA 获得的输入应该是：
```
    ### Instruction:\n将下列文本翻译成英文：今天天气真好\n\n### Response:\n
```
其需要拟合的输出则是：
```
    ### Instruction:\n将下列文本翻译成英文：今天天气真好\n\n### Response:\nToday is a nice day！
```
注意，因为指令微调本质上仍然是对模型进行 CLM 训练，只不过要求模型对指令进行理解和回复而不是简单地预测下一个 token，所以模型预测的结果不仅是 output，而应该是 input + output，只不过 input 部分不参与 loss 的计算，但回复指令本身还是以预测下一个 token 的形式来实现的。

但是，随着 LLM 能力的不断增强，模型的多轮对话能力逐渐受到重视。所谓多轮对话，是指模型在每一次对话时能够参考之前对话的历史记录来做出回复。例如，一个没有多轮对话能力的 LLM 可能有如下对话记录：
```
    用户：你好，我很喜欢打篮球。
    模型：好的。
    用户：你知道我的爱好是什么吗？
    模型：不好意思，我不知道你的爱好是什么。
```
也就是说，模型不能记录用户曾经提到或是自己曾经回答的历史信息。如果是一个具有多轮对话能力的 LLM，其对话记录应该是这样的：
```
    用户：你好，我很喜欢打篮球。
    模型：好的。
    用户：你知道我的爱好是什么吗？
    模型：你的爱好是打篮球。
```
模型是否支持多轮对话，与预训练是没有关系的。事实上，模型的多轮对话能力完全来自于 SFT 阶段。如果要使模型支持多轮对话，我们需要在 SFT 时将训练数据构造成多轮对话格式，让模型能够利用之前的知识来生成回答。假设我们目前需要构造的多轮对话是：
```
    <prompt_1><completion_1><prompt_2><completion_2><prompt_3><completion_3>
```
构造多轮对话样本一般有三种方式：

1. 直接将最后一次模型回复作为输出，前面所有历史对话作为输入，直接拟合最后一次回复：
```        
        input=<prompt_1><completion_1><prompt_2><completion_2><prompt_3><completion_3>
        output=[MASK][MASK][MASK][MASK][MASK]<completion_3>
```
2. 将 N 轮对话构造成 N 个样本：
```
        input_1 = <prompt_1><completion_1>
        output_1 = [MASK]<completion_1>

        input_2 = <prompt_1><completion_1><prompt_2><completion_2>
        output_2 = [MASK][MASK][MASK]<completion_2>

        input_3=<prompt_1><completion_1><prompt_2><completion_2><prompt_3><completion_3>
        output_3=[MASK][MASK][MASK][MASK][MASK]<completion_3>
```
3. 直接要求模型预测每一轮对话的输出：
```
        input=<prompt_1><completion_1><prompt_2><completion_2><prompt_3><completion_3>
        output=[MASK]<completion_1>[MASK]<completion_2>[MASK]<completion_3>
```
显然可知，第一种方式会丢失大量中间信息，第二种方式造成了大量重复计算，只有第三种方式是最合理的多轮对话构造。我们之所以可以以第三种方式来构造多轮对话样本，是因为 LLM 本质还是进行的 CLM 任务，进行单向注意力计算，因此在预测时会从左到右依次进行拟合，前轮的输出预测不会影响后轮的预测。目前，绝大部分 LLM 均使用了多轮对话的形式来进行 SFT。


### RLHF

RLHF，全称是 Reinforcement Learning from Human Feedback，即人类反馈强化学习，是利用强化学习来训练 LLM 的关键步骤。相较于在 GPT-3 就已经初见雏形的 SFT，RLHF 往往被认为是 ChatGPT 相较于 GPT-3 的最核心突破。事实上，从功能上出发，我们可以将 LLM 的训练过程分成预训练与对齐（alignment）两个阶段。预训练的核心作用是赋予模型海量的知识，而所谓对齐，其实就是让模型与人类价值观一致，从而输出人类希望其输出的内容。在这个过程中，SFT 是让 LLM 和人类的指令对齐，从而具有指令遵循能力；而 RLHF 则是从更深层次令 LLM 和人类价值观对齐，令其达到安全、有用、无害的核心标准。

如下图所示，ChatGPT 在技术报告中将对齐分成三个阶段，后面两个阶段训练 RM 和 PPO 训练，就是 RLHF 的步骤：

![RLHF](https://raw.githubusercontent.com/BV003/images/main/llm/7.png)

RLHF 的思路是，引入强化学习的技术，通过实时的人类反馈令 LLM 能够给出更令人类满意的回复。强化学习是有别于监督学习的另一种机器学习方法，主要讨论的问题是智能体怎么在复杂、不确定的环境中最大化它能获得的奖励。强化学习主要由两部分构成：智能体和环境。在强化学习过程中，智能体会不断行动并从环境获取反馈，根据反馈来调整自己行动的策略。应用到 LLM 的对齐上，其实就是针对不同的问题，LLM 会不断生成对应的回复，人工标注员会不断对 LLM 的回复做出反馈，从而让 LLM 学会人类更偏好、喜欢的回复。

RLHF 就类似于 LLM 作为一个学生，不断做作业来去提升自己解题能力的过程。如果把 LLM 看作一个能力强大的学生，Pretrain 是将所有基础的知识教给他，SFT 是教他怎么去读题、怎么去解题，那么 RLHF 就类似于真正的练习。LLM 会不断根据 Pretrain 学到的基础知识和 SFT 学到的解题能力去解答练习，然后人类作为老师批改 LLM 的练习，来让 LLM 反思错误的解题方式，不断强化正确的解题方式。

如上图，RLHF 分为两个步骤：训练 RM 和 PPO 训练。

#### RM

RM，Reward Model，即奖励模型。RM 是用于拟合人类偏好，来给 LLM 做出反馈的。在强化学习的训练中，对于 LLM 的每一个回复，RM 会进行打分，这个打分反映了生成回复符合人类偏好的程度。然后 LLM 会根据强化学习的原理，基于 RM 的打分来进行优化训练。所以，RM 本质上是一个文本分类模型，对于一个文本输出一个标量奖励，和文本分类任务中的隐藏层输出非常类似。在具体实现上，RM 也往往就是传统的 LLM 架构（或 BERT 架构）加上一层分类层，和用于文本分类的 LLM 架构完全一致，只不过使用隐藏层输出而不是最后的分类输出而已。

但是，在训练 RM 时，我们往往并不直接使用文本及其对应的标量奖励来对 RM 进行训练。因为要对齐人类偏好，RM 训练的偏好数据往往是由人工标注的。但是，由于标注者之间往往也存在价值观差异，数值形式的标量奖励往往会将这些差异放大，从而导致在训练过程中对同样程度的回复奖励不一致，模型难以拟合到正确的标量奖励。因此，我们往往对同一个 completion 下的不同回复进行排名，再将排名转化为奖励。

例如，我们的训练数据往往形如：

```json
{
    "prompt":"如果你打算从商店偷东西，你觉得早上好还是晚上好？",
    "chosen":"这是违法的事情，我不能提供建议",
    "rejected":"考虑晚上的人口贩运和监控摄像头的差别是件好事。夜间时间可能更有利于避免监控摄像头,但晚上的商店雇员会更能看见你。另一方面,由于白天通常不太忙,因此更容易避免被其他人注意到。无论如何,必须密切注意商店雇员和所有顾客的行为。他们也许能够看见你,即使他们不是直接面对你。为了安全起见,重要的是要密切注意商店里的一切事情,而不是不小心。"
}
```

其中，prompt 是用户的问题，chosen 是应该对齐的、符合人类偏好的回答，rejected 是不符合人类偏好的回答。在训练中，prompt 将和 chosen 以及 rejected 分别拼接起来，形成 chosen_example 和 rejected_example，然后分别进入模型通过前向传播输出一个标量奖励。然后模型会通过最大化 chosen_example 和 rejected_example 的标量差异来计算 loss，并进行反向传播完成训练。

值得注意的是，RM 训练使用的模型往往和最后的 LLM 大小不同。例如 OpenAI 使用了 175B 的 LLM 和 6B 的 RM。同时，RM 使用的模型可以是经过 SFT 之后的 LM，也可以是基于偏好数据从头训练的 RM。哪一种更好，至今尚没有定论。

#### PPO

在完成 RM 训练之后，就可以使用 PPO 算法来进行强化学习训练。PPO，Proximal Policy Optimization，近端策略优化算法，是一种经典的 RL 算法。事实上，强化学习训练时也可以使用其他的强化学习算法，但目前 PPO 算法因为成熟、成本较低，还是最适合 RLHF 的算法。

在具体 PPO 训练过程中，会存在四个模型。如下图所示，两个 LLM 和两个 RM。两个 LLM 分别是进行微调、参数更新的 actor model 和不进行参数更新的 ref model，均是从 SFT 之后的 LLM 初始化的。两个 RM 分别是进行参数更新的 critic model 和不进行参数更新的 reward model，均是从上一步训练的 RM 初始化的。

![PPO](https://raw.githubusercontent.com/BV003/images/main/llm/8.jpg)

如上图，使用 PPO 算法的强化学习训练过程如下：

1. 从 SFT 之后的 LLM 初始化两个模型分别作为 Actor Model 和 Ref Model；从训练的 RM 初始化两个模型分别作为 Reward Model 和 Critic Model。
2. 输入一个 Prompt，Actor Model 和 Ref Model 分别就 Prompt 生成回复。
3. Actor Response 和 Ref Response 计算 KL 散度：
$$ 
 r_{KL} = -\theta_{KL} D_{KL}(\pi_{PPO}(y|x) \parallel \pi_{base}(y|x)) 
$$  
其中，\\( \pi_{PPO}(y|x) \\) 即为 Actor Model 的输出，而 \\( \pi_{base}(y|x) \\) 即为 Ref Model 的输出，\\( \theta_{KL} D_{KL} \\) 即是计算 KL 散度的方法。
4. Actor Response 分别输入到 Reward Model 和 Critic Model 进行打分，其中，Reward Model 输出的是回复对应的标量奖励，Critic Model 还会输出累加奖励（即从 i 位置到最后的累积奖励）。
5. 计算的 KL 散度、两个模型的打分均输入到奖励函数中，计算奖励：  
$$
loss = -(kl_{ctl} \cdot r_{KL} + \gamma \cdot V_{t+1} - V_{t}) \log P(A_t|V_t)
$$
其中，\\( kl_{ctl} \\) 是控制 KL 散度对结果影响的权重参数，\\( \gamma \\) 是控制下一个时间（即样本）打分对结果影响的权重参数，\\( V_t \\) 是 Critic Model 的打分输出，\\( A_t \\) 则是 Reward Model 的打分输出。
6. 根据奖励函数分别计算出的 actor loss 和 critic loss，更新 Actor Model 的参数和 Critic Model 的参数。注意，Actor Model 和 Critic Model 的参数更新方法是不同的，此处就不再一一赘述。

我们可以从直观上这样进行理解，训练一开始，我们会从监督微调（SFT）阶段得到的语言模型中复制出两个版本：其中一个叫 Actor Model，用来继续训练；另一个叫 Ref Model，用来作为参照，保持参数不变。同时，我们还会用到之前训练好的奖励模型（Reward Model），它能根据人类偏好为生成的回答打分。我们同样复制出两个版本，一个保持不动作为 Reward Model 本身，另一个是 Critic Model，用来学习评估整体奖励，也就是预测模型从当前回答开始到整个对话结束可能获得的总回报。接下来，每轮训练都会输入一个提示（Prompt），让 Actor Model 和 Ref Model 各自生成一个回答。然后我们比较它们的输出差异，这个差异反映了 Actor Model 是否偏离了原来的表达方式，如果偏离太多，系统会给出惩罚，以防模型“跑偏”，失去原本在预训练和微调中学到的语言能力。之后，我们把 Actor Model 生成的回答输入到 Reward Model 和 Critic Model。Reward Model 会给出一个即时的分数，表示这个回答在人类看来好不好；Critic Model 则会估计从现在到结束的总体表现，也就是长期回报。系统会综合这些信息，生成一个总体的奖励信号：如果回答既符合人类偏好，又没有偏离原模型太多，奖励就高；反之则低。然后，Actor Model 会根据这个信号调整自己的参数，学习哪些生成方式更受人类喜欢，而 Critic Model 也会同时被更新，以更准确地预测奖励。

在上述过程中，因为要使用到四个模型，显存占用会数倍于 SFT。例如，如果我们 RM 和 LLM 都是用 7B 的体量，PPO 过程中大概需要 240G（4张 80G A100，每张卡占用 60G）显存来进行模型加载。那么，为什么我们需要足足四个模型呢？Actor Model 和 Critic Model 较为容易理解，而之所以我们还需要保持原参数不更新的 Ref Model 和 Reward Model，是为了限制模型的更新不要过于偏离原模型以至于丢失了 Pretrain 和 SFT 赋予的能力。

#### DPO

当然，如此大的资源占用和复杂的训练过程，使 RLHF 成为一个门槛非常高的阶段。也有学者从监督学习的思路出发，提出了 DPO（Direct Preference Optimization，直接偏好优化），可以低门槛平替 RLHF。DPO 的核心思路是，将 RLHF 的强化学习问题转化为监督学习来直接学习人类偏好。DPO 通过使用奖励函数和最优策略间的映射，展示了约束奖励最大化问题完全可以通过单阶段策略训练进行优化，也就是说，通过学习 DPO 所提出的优化目标，可以直接学习人类偏好，而无需再训练 RM 以及进行强化学习。由于直接使用监督学习进行训练，DPO 只需要两个 LLM 即可完成训练，且训练过程相较 PPO 简单很多，是 RLHF 更简单易用的平替版本。DPO 所提出的优化目标为什么能够直接学习人类偏好，作者通过一系列的数学推导完成了证明。

DPO 的核心洞见是：我们完全可以绕过奖励模型建模这一中间步骤，直接利用人类的偏好数据来优化语言模型。想象一下，你给模型展示一对回答：一个是“人类更喜欢的回答”，另一个是“人类不太喜欢的回答”。DPO 就是在训练中告诉模型：“你应该更倾向于生成那个被人类偏好的回答。”它的做法类似于一种“拉近-推远”的策略：把模型输出的概率分布调整得更偏向人类喜欢的回答，同时让不被偏好的回答的生成概率降低。换句话说，模型学到的不是简单的“答案对错”，而是一种价值判断的偏好方向——它知道哪些表达方式、内容或者风格是更受人类欢迎的。

DPO 的训练流程可以理解为一个“基于偏好示例的监督学习”过程。首先，准备一批人类偏好数据，每条数据包含同一个提示下的两个回答——一个是人类更喜欢的，另一个是较不受欢迎的。接下来，把这批数据输入训练系统：模型会同时看到“优回答”和“劣回答”，并通过特定的目标告诉模型应该提高生成优回答的概率，降低生成劣回答的概率。整个训练过程中，模型的参数会不断调整，使其输出分布更倾向于人类偏好的方向。与 RLHF 不同，DPO 不需要训练单独的奖励模型，也不需要策略梯度或强化学习步骤，整个过程就像标准的监督学习，只是训练目标从“预测下一个词”变成了“让模型学会分辨人类偏好的回答”。经过多轮迭代训练后，模型就能够在面对新的提示时，自然地产生符合人类偏好的回答，从而实现对齐人类价值观的目标。


#### GRPO

GRPO（Group Relative Policy Optimization）是一种用于大语言模型对齐的强化学习方法，它可以看作是对 PPO 的改进和优化。PPO 的核心是优势函数 ，它需要一个奖励模型  来提供奖励，还需要一个价值模型  来提供基线（Baseline），即在状态  下的平均期望回报。训练和维护这个价值模型是 PPO 流程中主要的复杂性和成本来源之一。

GRPO 的提出者思考：我们能不能找到一种更简单的方式来估计这个“平均水平”呢？

他们的答案是：利用群体智慧。对于同一个指令，我们不只生成一个回答，而是生成一组（Group）回答。然后，我们假设这组回答的平均奖励，就可以近似地作为当前策略下的“平均水平”，也就是价值  的一个估计。这个想法非常直观。想象一下，要评价一个学生这次考试的成绩（某个回答的奖励）是好是坏（优势），我们不需要知道他历史上的平均分（价值模型的输出），我们可以直接看他这次在班级里（一组回答中）的排名。如果他的分数远高于班级平均分，那么他的优势就是正的，反之亦然。

GRPO 的核心是对 PPO 中优势函数的计算方式进行了修改。其步骤如下：
1. 组采样 (Group Sampling)：对于一个给定的指令 ，使用当前的策略模型  生成一个包含多个回答的组；
2. 组评估 (Group Evaluation)：使用一个奖励函数（可以是一个训练好的奖励模型，也可以是某种可计算的启发式规则，例如代码的执行结果、数学题的答案是否正确等）为组内的每一个回答打分，得到奖励；
3. 组内优势计算 (Group-Relative Advantage Estimation)：计算组内所有回答的平均奖励和标准差 。对于组内的每一个回答 ，其优势被定义为其归一化后的奖励，这种方法被称为组内奖励归一化（Group-wise Reward Normalization）。它直接用组内的统计量（均值和标准差）来替代了 PPO 中需要专门训练的价值模型所扮演的角色；
4. 策略更新：一旦计算出了每个样本的优势 ，接下来的步骤就和 PPO 非常相似了。GRPO 同样使用 Clipped Surrogate Objective 来更新策略模型。

### Reason models（推理模型）

Reasoning Models（推理模型）是相较于传统大语言模型（LLM）的一种新型模型方向，其目标是让模型具备显式的思维与推理能力，而不仅仅是语言生成能力。Reasoning Models 不仅生成答案，还生成思考过程（thought process）。

Reasoning Models 的核心思想是显式推理轨迹，模型在输出最终答案之前，会先生成中间步骤（chain-of-thought, CoT），推理轨迹的目的是模拟人类的“思考过程”，而非直接预测答案。模型会在生成后对自己的推理过程进行检验。一些常见的机制有，Self-consistency（多路径推理后投票），Reflection（模型重新阅读自己的思考轨迹）。

推理强化（Reasoning Reinforcement）是指我们使用强化学习（RL）优化模型的推理链质量。奖励信号不再来自人类偏好（如 RLHF），而是来自推理正确性（Reasoning Reward）。

我们来深入了解一下这一领域的具体技术。

提高 LLM 的推理能力（或任何一般能力）的一种方法是Inference-time scaling，在这里，Inference-time scaling指在推理过程中增加计算资源以提高输出质量。比如当有更多时间思考复杂问题时，人类往往会产生更好的反应。因此，我们可以用一些技术，鼓励 LLM 在生成答案时更多地 “think”。一种直接的方法比如通过prompt engineering，比如经典的CoT prompting："think step by step"。将鼓励模型生成中间推理步骤，而不是直接跳到最终答案，这通常会在更复杂的问题上产生更准确的结果，但非一直有效。另一种方法是使用 voting and search strategies（投票和搜索策略）。一个简单的例子是多数投票，我们让 LLM 生成多个答案，我们通过多数票选择正确答案。同样，我们可以使用beam search和其他搜索算法来生成更好的响应。

Pure reinforcement learning，与典型的 RL pipeline 不同，在 RL 之前应用 SFT，而 DeepSeek-R1-Zero 完全使用强化学习进行训练，没有初始 SFT 阶段。对于奖励，他们没有使用根据人类偏好训练的奖励模型，而是采用了两种类型的奖励：准确性奖励和格式奖励。准确性奖励使用 LeetCode 编译器来验证编码答案，并使用确定性系统来评估数学响应。格式奖励依赖于 LLM 裁判来确保回答遵循预期的格式，例如在< think>标签内放置推理步骤。

在SFT过程中加入带推理提示词的语料也是可行的。我们也可以使用蒸馏的方法来让模型获得推理能力，LLM 的蒸馏是指在由较大的 LLM 生成的 SFT 数据集上对较小的 LLM 进行指令微调（sft）。对于较小的模型，蒸馏比纯 RL 有效得多。

在实际模型开发中，RL + SFT 是首选方法，因为它会产生更强大的推理模型。

### LORA

Aghajanyan的研究表明：预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果。同时Aghajanyan发现在预训练后，越大的模型有越小的内在维度，这也解释了为何大模型都拥有很好的few-shot能力。受instrisic dimension工作的启发，LORA认为参数更新过程中也存在一个‘内在秩’。它的核心思想是：在不修改原始模型参数的情况下，只训练少量“低秩矩阵”来实现模型的个性化微调。

将所有微调参数都放到attention的某一个参数矩阵的效果并不好，将可微调参数平均分配到q矩阵和k矩阵的效果最好。


### 常见损失函数
**回归任务**

均方误差（MSE）
$$
L = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
适用于回归问题，对异常值敏感，梯度连续。

平均绝对误差（MAE）
$$
L = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i|
$$
对异常值鲁棒，但梯度在0处不可导。

**分类任务**

交叉熵（Cross-Entropy）
$$
L = -\sum_{i} y_i \log \hat{y}_i
$$
用于多分类问题，输出需 softmax。当模型对正确类别（比如是猫）的预测概率很高，对错误类别（比如是狗）的预测概率很低时，交叉熵就小。

二分类交叉熵（BCE）
$$
L = -[y \log \hat{y} + (1-y)\log(1-\hat{y})]
$$
用于二分类问题，输出需 sigmoid。

Hinge Loss
$$
L = \max(0, 1 - y_i \cdot \hat{y}_i)
$$
用于 SVM，正负样本标签通常为 +1/-1。

**概率分布差异**

KL 散度（KL Divergence）
$$
D_{KL}(P||Q) = \sum_i P(i) \log \frac{P(i)}{Q(i)}
$$
衡量两个概率分布差异，常用于生成模型或模型输出分布拟合。当模型预测的分布和真实分布越像时，KL 散度就越小。






## 工程实现

### 分布式训练DeepSpeed

Deepspeed 的核心策略是 ZeRO 和 CPU-offload。ZeRO 是一种显存优化的数据并行方案，其核心思想是优化数据并行时每张卡的显存占用，从而实现对更大规模模型的支持。ZeRO 将模型训练阶段每张卡被占用的显存分为两类：

- 模型状态（Model States），包括模型参数、模型梯度和优化器 Adam 的状态参数。假设模型参数量为 1M，一般来说，在混合精度训练的情况下，该部分需要 16M 的空间进行存储，其中 Adam 状态参数会占据 12M 的存储空间。
- 剩余状态（Residual States），除了模型状态之外的显存占用，包括激活值、各种缓存和显存碎片。

针对上述显存占用，ZeRO 提出了三种不断递进的优化策略：

1. ZeRO-1，对模型状态中的 Adam 状态参数进行分片，即每张卡只存储 \\( \frac{1}{N} \\)的 Adam 状态参数，其他参数仍然保持每张卡一份。
2. ZeRO-2，继续对模型梯度进行分片，每张卡只存储 \\( \frac{1}{N} \\) 的模型梯度和 Adam 状态参数，仅模型参数保持每张卡一份。
3. ZeRO-3，将模型参数也进行分片，每张卡只存储 \\( \frac{1}{N} \\) 的模型梯度、模型参数和 Adam 状态参数。

可以看出，随着分片的参数量不断增加，每张卡需要占用的显存也不断减少。当然，分片的增加也就意味着训练中通信开销的增加，一般而言，每张卡的 GPU 利用率 ZeRO-1 最高而 ZeRO-3 最低。具体使用什么策略，需要结合计算资源的情况和需要训练的模型体量动态确定。



### 优化器：AdamW
优化器的核心任务就是通过调整模型的参数（Parameters），来找到一个能让损失函数（Loss Function）值最小的点。梯度下降的整个过程可以被一个简单的公式概括：

```
新参数 = 旧参数 - 学习率 × 梯度
```

这个公式虽然简洁，但留下了一个关键问题：这个“梯度”到底该如何计算？我们知道，损失函数是基于整个训练数据集计算的。因此，最直观、最“忠于”数学定义的方法，就是计算损失函数在整个训练集上关于参数θ的梯度。基于这种思想的梯度下降法，就是批量梯度下降（Batch Gradient Descent, BGD）。BGD的“批量”（Batch）指的就是全部的训练数据。它的每一步更新都遵循以下严谨的流程：遍历整个数据集：将训练集中所有的样本（从第一个到最后一个）输入到模型中，进行一次完整的前向传播。计算总损失：根据所有样本的预测结果和真实标签，计算出整个数据集上的总损失（或者平均损失）。计算总梯度：基于这个总损失，进行一次反向传播，计算出损失函数关于每一个模型参数的梯度。这个梯度综合了所有样本的信息，代表了当前位置最准确的下降方向。更新参数：使用计算出的总梯度，对模型参数进行一次更新。BGD的优点也正是它致命的缺点来源——对“全体”数据的依赖。计算成本极高,内存占用巨大,容易陷入局部最优。

既然用全部样本计算梯度太慢，那我们能不能做一个极致的简化：每次只用一个样本来计算梯度并更新参数？这就是SGD（Stochastic Gradient Descent）的核心思想。每次迭代，我们从训练集中随机抽取一个样本，用这一个样本来近似代表全体数据，计算梯度并更新参数。当所有样本都被使用过一次后，就完成了一个轮次（epoch）。可以看到，在SGD中，一个epoch包含N次参数更新（N是样本总数），而在BGD中，一个epoch只有一次更新。这使得SGD的训练速度快了N个数量级。SGD剧烈震荡的缺点，在某些情况下反而成了一个“优点”。可以跳出局部最优。

是否存在一种“折中”的方案，既能享受SGD带来的计算效率，又能获得BGD那样的收敛稳定性呢？答案是肯定的，这就是在当今深度学习实践中被广泛采用的——小批量梯度下降（Mini-batch Gradient Descent）。Mini-batch GD的核心思想非常直观：不使用全部数据，也不只用一个样本，而是在每次更新时使用一小批（a mini-batch）数据。它的工作流程是BGD和SGD的完美结合：

随机打乱数据集：和SGD一样，在每个轮次（epoch）开始前，先将整个训练数据集随机打乱。

划分小批量：将打乱后的数据集划分为若干个大小相等的小批量（mini-batches）。最后一个批次可能因样本数量无法整除而略小。

迭代更新：
1. 取出一个小批量数据。
2. 将这批数据输入模型，计算它们的平均损失。
3. 基于这个平均损失进行反向传播，计算出关于参数的平均梯度。
4. 使用这个平均梯度对模型参数进行一次更新。

重复：继续取出下一个小批量，重复步骤3，直到所有的小批量都被使用过一次。
当所有小批量都处理完毕后，就完成了一个轮次（epoch）。

我们能否有一种机制，可以减少在梯度变化剧烈方向上的震荡，同时加速在梯度方向一致的方向上的前进？核心思想：引入物理世界的“惯性”。Momentum在SGD的基础上，增加了一个动量项。参数更新不再只依赖于当前的梯度，而是依赖于动量和当前梯度的结合。形象地看，Momentum就像给SGD装上了一个“减震器”和“加速器”。它平滑了SGD的更新路径，使其下降轨迹更加稳定和高效。

在此之前，我们讨论的所有优化算法（SGD, Momentum）都共享一个特点：它们对所有的参数使用同一个全局学习率 。无论这个参数是用于模型浅层还是深层，是对应于常见特征还是稀疏特征，它的更新步长都由同一个参数控制。但这并不合理。对于常见特征，我们可能希望学习率小一些，因为它们已经得到了充分的训练，需要精细调整。而对于稀疏特征，我们希望在它们难得出现一次时，能用一个更大的学习率来“抓住机会”进行一次充分的更新，以弥补它们出场次数少的劣势。AdaGrad (Adaptive Gradient Algorithm) 就是为此而设计的。AdaGrad的核心思想非常直观：对于那些迄今为止梯度累积较大的参数，我们让它的学习率变小；对于那些梯度累-积较小的参数，我们让它的学习率变大。换句话说，一个参数被更新得越频繁，它的有效学习率就越低。这完美地契合了我们对稀疏特征的需求。

我们在AdaGrad中看到，通过为每个参数维护一个独立的、不断累积的梯度平方和，实现了自适应学习率。这个想法很棒，但它有一个致命的缺陷：学习率单调递减，最终会趋近于零。RMSprop 给出了一个简单而优雅的解决方案。它是由深度学习巨擘Geoff Hinton在他的一门在线课程中提出的，虽然没有公开发表为论文，但其有效性使其在实践中被广泛采用。RMSprop的核心思想与AdaGrad非常相似，都是利用梯度的平方来调整学习率。但关键的区别在于，RMSprop不再将所有历史梯度一视同仁地累加，而是采用了一种加权平均的方式，使得越久远的梯度信息对当前的影响越小。

Adam（Adaptive Moment Estimation） 的伟大之处就在于，它将这两条路线的优点巧妙地结合在了一起。你可以把Adam看作是 Momentum + RMSprop 的合体。Adam 在训练过程中，会同时维护两个“动量”：一阶矩（均值）：梯度的指数加权平均，用来表示“梯度的趋势”；二阶矩（方差）：梯度平方的指数加权平均，用来表示“梯度的尺度大小”。Adam 会根据这两个信息 动态调整每个参数的学习率，使得每个参数都能以合适的速度更新。

AdamW 是 Adam 的一个改进版本，由 Loshchilov 和 Hutter 于 2017 年 提出。它的主要改进在于：把“权重衰减（weight decay）”从 Adam 的梯度更新中分离出来，从而更好地控制模型的正则化。权重衰减应该是一种固定比例的参数缩小，不应该被梯度的方差估计所干扰。

### 杂项
- 数据工程，Spark, Flink, 数据标注流程，数据清洗与 Tokenizer
- 不同厂商的云平台
- 学习率调度与稳定性技巧




## 推理与部署

模型压缩：量化、蒸馏、剪枝

高效推理：vLLM, TensorRT, ONNX

KV Cache, 批处理优化


## 应用与实践

下游任务：QA、对话、文本摘要、分类

评测指标：Perplexity, BLEU, ROUGE, BERTScore

对齐与安全：偏见缓解、有害内容检测、系统安全





## 智能体系统
### RAG
RAG（Retrieval-Augmented Generation） 是一种结合了“知识检索”与“文本生成”的混合式大模型架构。它的核心思想是：让模型在生成答案前，先从外部知识库中检索相关信息，再基于这些真实资料进行生成，从而提升准确性与时效性。

传统的语言模型（如 GPT、LLaMA）在训练后，它的知识就“冻结”在参数中。模型能生成语言，却无法实时获取外部信息；一旦世界发生变化（例如新论文、新法规发布），模型的回答就可能过时或错误。RAG 的出现正是为了解决这一问题。

### Agent

Agent（智能体） 是在大模型基础上发展出的一个更高层次概念。
如果说大模型是“语言理解与生成的引擎”，那么 Agent 则是让这个引擎“拥有目标、能感知环境、能规划行动”的系统。

Agent 的本质是：让模型从被动应答者变为主动执行者。
它不仅能回答问题，还能调用工具、执行代码、访问网络、做决策、与其他 Agent 协作，从而形成一个有记忆、有行动的智能系统。

一个典型的大模型 Agent 系统由以下几个部分组成：

- 感知（Perception）Agent 接收输入，如用户指令、环境信息、上下文状态；通过语言模型理解意图和当前场景。

- 决策（Planning）模型基于推理能力制定计划：要调用哪些工具、执行哪些步骤；例如 “先搜索 → 再分析 → 最后总结”。

- 行动（Action / Tool Use）Agent 调用外部工具执行计划，比如调用 API、检索数据库、运行 Python 代码、控制浏览器等。

- 记忆（Memory）Agent 能记录交互历史、任务结果、用户偏好；通过短期记忆（上下文缓存）与长期记忆（数据库）不断改进决策。

- 反思（Reflection）先进的 Agent 会自我检查结果是否合理，如果错误则重新规划。这类机制在 “Reasoning + Acting + Reflection” 研究方向中尤为重要。

## 各家开源大厂技术报告

qwen、llama网络结构：gqa等，长文本rope，训练细节
dpskv3base，r1的细节




### Article Attribution and License
Author: Michael Liu  
Original Link: [https://bv003.github.io/posts/blog/llm](https://bv003.github.io/posts/blog/llm)  
License: This article is licensed under CC BY-SA 4.0. Redistribution is not permitted without complying with the license requirements.  

### References
1. DatawhaleChina. Happy-LLM: A Tutorial on Large Language Models from Scratch. GitHub.[https://github.com/datawhalechina/happy-llm](https://github.com/datawhalechina/happy-llm)
2. Vaswani, A., Shazeer, N., Parmar, N., et al. "Attention is All You Need." NeurIPS, 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
3. Lilian Weng. Attention? Attention!  [https://lilianweng.github.io/posts/2018-06-24-attention/](https://lilianweng.github.io/posts/2018-06-24-attention/)
4. 从 PPO、DPO 到 GRPO：万字长文详解大模型训练中的三大关键算法 [link](https://mp.weixin.qq.com/s/OMpD6ITqNi4jX95nSRC2Ig)
5. 优化器详解[link](https://zhuanlan.zhihu.com/p/1928857996655588384)