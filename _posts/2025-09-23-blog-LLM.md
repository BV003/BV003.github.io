---
title: 'Collection of Information on Large Language Models'
date: 2025-09-23
permalink: /posts/blog/llm
---

To prepare for an MLsys-related internship, I have organized some relevant resources. I'm sharing them here in the hope that they may help others as well.


<!-- excerpt -->

## Transformer
#### Transformer介绍 
Transformer 诞生于 2017 年，由 Vaswani 等人在论文《Attention Is All You Need》中首次提出。和传统网络RNN，CNN相比，Transformer具有极大的优势。Transformer 采用完全基于注意力机制（Attention）的架构，通过自注意力（Self-Attention）机制能够直接捕捉序列中任意位置的依赖。借助多头注意力（Multi-Head Attention）和位置编码（Positional Encoding）Transformer迅速取代 RNN 和 CNN，成为现代 NLP 的核心基础模型。


#### Model Architecture （编码器和解码器）
![模型架构](https://raw.githubusercontent.com/BV003/images/main/llm/0.png)
![模型架构](https://raw.githubusercontent.com/BV003/images/main/llm/1.png)

编码器由𝑁=6层相同的层堆叠而成。每一层包含两个子层。第一个子层是多头自注意力机制（multi-head self-attention），第二个子层是简单的逐点全连接前馈网络（position-wise fully connected feed-forward network）。我们在每个子层周围使用残差连接（residual connection），随后进行层归一化（layer normalization）。也就是说，每个子层的输出为

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

解码器（Decoder）：解码器同样由 N=6 层相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器还插入了第三个子层，对编码器堆栈的输出进行多头注意力计算（multi-head attention）。与编码器类似，我们在每个子层周围使用残差连接，并进行层归一化。

残差连接就是把每个子层的输入直接加到子层输出上，再经过归一化，这样既保留了原始信息，又缓解了深层网络的梯度消失问题，使训练更稳定、收敛更快。

我们还修改了解码器堆栈中的自注意力子层，使得每个位置无法关注后续位置。这种掩码（masking）结合输出嵌入向量偏移一个位置的方式，确保位置 \\( i \\) 的预测只能依赖于位置小于 \\( i \\) 的已知输出。

#### Attention 机制
注意力（attention）函数可以被描述为：将一个查询（query）和一组键值对（key-value pairs）映射为一个输出，其中查询、键、值以及输出都是向量。输出通过对值向量的加权求和得到，而每个值的权重由查询与对应键之间的兼容性函数（compatibility function）计算得出。

我们可以定义缩放点积注意力（Scaled Dot-Product Attention）。输入包括维度为 \\( d_k \\) 的查询（queries）和键（keys），以及维度为 \\( d_v \\) 的值（values）。我们先计算查询与所有键的点积，然后除以 \\(  \sqrt{d_k} \\)，再通过 softmax 函数得到各值向量的权重（Softmax 是一种常用的归一化函数）。


在实际操作中，我们通常同时对一组查询计算注意力函数，并将其打包成矩阵 
\\(Q\\)。键和值也分别打包成矩阵 \\(K\\) 和 \\(V\\)。输出矩阵的计算公式为：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

我们引入通过 \\(  \sqrt{d_k} \\) 的目的是防止点积过大导致 softmax 的梯度变得非常小，保证训练稳定性。

```
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    """
    实现 Scaled Dot-Product Attention
    公式: Attention(Q, K, V) = softmax((Q K^T)/√d_k) V
    """

    def __init__(self, input_dim, dim_k, dim_v):
        """
        Args:
            input_dim (int): 输入特征维度（d_model）
            dim_k (int): 查询和键的维度
            dim_v (int): 值的维度
        """
        super().__init__()
        
        # 线性映射，将输入映射到 Q, K, V
        self.q = nn.Linear(input_dim, dim_k)  # 查询
        self.k = nn.Linear(input_dim, dim_k)  # 键
        self.v = nn.Linear(input_dim, dim_v)  # 值
        
        # 缩放因子 √d_k
        self.scale = math.sqrt(dim_k)

    def forward(self, x):
        """
        Args:
            x: 输入张量, 维度 (batch_size, seq_len, input_dim)
        Returns:
            output: 注意力输出, 维度 (batch_size, seq_len, dim_v)
        """
        # 1. 线性映射到 Q, K, V
        # 输出维度: (batch_size, seq_len, dim_k) 或 (batch_size, seq_len, dim_v)
        Q = self.q(x)
        K = self.k(x)
        V = self.v(x)

        # 2. 计算注意力分数
        # 使用 matmul 可以直接对 3D 张量做 batch 矩阵乘法
        # K.transpose(-2, -1) 将 (batch, seq_len, dim_k) -> (batch, dim_k, seq_len)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (batch, seq_len, seq_len)

        # 3. 对注意力分数做 softmax，得到注意力权重
        attn_weights = torch.softmax(scores, dim=-1)  # 每行归一化

        # 4. 注意力权重乘以 V
        output = torch.matmul(attn_weights, V)  # (batch, seq_len, dim_v)

        return output
```


#### Multi-Head Attention (MHA)

![多头注意力](https://raw.githubusercontent.com/BV003/images/main/llm/2.png)

我们发现，将查询、键和值分别通过 h 次不同的、可学习的线性映射投影到 \\( d_k \\)、\\( d_k \\) 和 \\( d_v \\) 维度上更为有效。在这些投影后的查询、键和值上，我们并行执行注意力函数，得到 \\( d_v \\) 维度的输出值。

多头注意力允许模型同时关注来自不同表示子空间的不同位置的信息。而单头注意力在平均操作下会抑制这一能力。

公式表示为：

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

其中：

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

投影矩阵为参数矩阵：

$$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$$

$$W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$$

$$W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$$

$$W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}}$$


在本工作中，我们使用 h = 8 个并行注意力层（即头）。对于每个头，我们设置 \\( d_k \\) = \\( d_v \\) =\\( d_{\text{model}} \\) /h = 64。由于每个头的维度较小，总计算成本与使用全维度的单头注意力相当。

```
import torch
import torch.nn as nn
import math

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, input_dim, num_heads, dim_k, dim_v):
        """
        Multi-Head Self-Attention
        
        Args:
            input_dim (int): 输入特征维度（d_model）
            num_heads (int): 注意力头数量（h）
            dim_k (int): 所有头的总查询/键维度（通常 = input_dim）
            dim_v (int): 所有头的总值维度（通常 = input_dim）
        """
        super().__init__()
        assert dim_k % num_heads == 0, "dim_k must be divisible by num_heads"
        assert dim_v % num_heads == 0, "dim_v must be divisible by num_heads"

        self.num_heads = num_heads
        self.dim_per_head_k = dim_k // num_heads
        self.dim_per_head_v = dim_v // num_heads

        # Q、K、V 的线性映射层
        self.q_linear = nn.Linear(input_dim, dim_k)
        self.k_linear = nn.Linear(input_dim, dim_k)
        self.v_linear = nn.Linear(input_dim, dim_v)

        # 输出线性映射，把拼接后的多头输出映射回 input_dim
        self.out_linear = nn.Linear(dim_v, input_dim)

        # 缩放因子 √d_k
        self.scale = math.sqrt(self.dim_per_head_k)

    def forward(self, x):
        """
        x: (batch_size, seq_len, input_dim)
        """
        b, s, _ = x.size()

        # 1. 线性映射到 Q, K, V
        # 输出维度: (b, s, num_heads * dim_per_head)
        Q = self.q_linear(x)
        K = self.k_linear(x)
        V = self.v_linear(x)

        # 2. reshape 并转置为多头: (b, num_heads, s, dim_per_head)
        # view拆分最后一位
        Q = Q.view(b, s, self.num_heads, self.dim_per_head_k).transpose(1, 2)
        K = K.view(b, s, self.num_heads, self.dim_per_head_k).transpose(1, 2)
        V = V.view(b, s, self.num_heads, self.dim_per_head_v).transpose(1, 2)

        # 3. 计算注意力分数
        # Q * K^T / sqrt(d_k)
        # Q: (b, h, s, d_k), K: (b, h, s, d_k) -> (b, h, s, s)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale

        # 4. softmax 得到注意力权重
        attn_weights = torch.softmax(scores, dim=-1)

        # 5. 权重乘以 V 得到每个头的输出: (b, h, s, dim_per_head_v)
        attn_output = torch.matmul(attn_weights, V)

        # 6. 转置并拼接多头输出: (b, s, num_heads * dim_per_head_v)
        attn_output = attn_output.transpose(1, 2).contiguous().view(b, s, -1)

        # 7. 输出线性映射回 input_dim: (b, s, input_dim)
        out = self.out_linear(attn_output)

        return out

```
Masked 版本在 softmax 前增加一个 mask：

$$
\text{MaskedAttention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} + M \right) V
$$

其中 $$M \in \mathbb{R}^{\text{seq_len} \times \text{seq_len}}$$ 是上三角掩码矩阵

掩码元素：

$$
M_{i,j} =
\begin{cases}
0, & j \leq i \\
-\infty, & j > i
\end{cases}
$$

这样 softmax 后，未来 token 的注意力权重被 置为 0，模型无法访问未来信息。

#### Feed-Forward Network, FFN
FFN是MLP多层感知机的一种。  
除了注意力子层之外，我们的编码器和解码器中的每一层还包含一个 全连接前馈网络，它会分别并且一致地应用于序列中的每一个位置。  
这个网络由两个线性变换和中间的一个 ReLU 激活函数组成：

$$
\text{FFN}(x) = \max\left(0, xW_1 + b_1\right)W_2 + b_2
$$

尽管这些线性变换在不同位置上是相同的，但在不同层之间它们的参数是不同的。
输入和输出的维度为\\( d_{\text{model}} \\)=512，而内部层的维度为\\( d_{\text{ff}} \\)=2048。

```
import torch
import torch.nn as nn

class FFN(nn.Module):
    def __init__(self, d_model=512, d_ff=2048, dropout=0.0):
        super().__init__()
        # 等价于两个线性层和中间的 ReLU
        self.w1 = nn.Linear(d_model, d_ff)   # x @ W1^T + b1  （PyTorch Linear 自带偏置）
        self.w2 = nn.Linear(d_ff, d_model)   # hidden @ W2^T + b2
        self.relu = nn.ReLU()
        # dropout为丢弃神经元的概率
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        hidden = self.w1(x)       # (batch, seq, d_ff)
        hidden = self.relu(hidden)
        hidden = self.dropout(hidden)
        out = self.w2(hidden)     # (batch, seq_len, d_model)
        return out
```

#### Embeddings and Softmax
我们使用 可学习的嵌入（learned embeddings） 将输入和输出的 token 转换为维度为 \\( d_{\text{model}} \\)的向量。  
同时，我们也使用通常的可学习线性变换（linear transformation）和softmax函数，将解码器的输出转换为预测的下一个 token的概率分布。  
在我们的模型中，两个嵌入层（输入嵌入和输出嵌入）以及 softmax 前的线性变换共享相同的权重矩阵。
在嵌入层中，我们还会将这些权重乘以 \\( \sqrt{d_{\text{model}}} \\)进行缩放。

#### Positional Encoding
我们在编码器和解码器堆栈底部的输入嵌入中，加入了位置编码（positional encodings）。位置编码的维度与嵌入维度相同\\( d_{\text{model}} \\)，以便可以直接与嵌入向量相加。

$$
\begin{align*}
\text{PE}(pos, 2i) &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
\text{PE}(pos, 2i + 1) &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{align*}
$$

其中 \\( pos\\)表示位置，\\(i\\)表示维度索引。换句话说，每一个维度对应一个正弦或余弦函数。

#### Layer Normalization
归一化的目的主要是加速训练、稳定梯度、改善收敛性，缓解超参数敏感性。LayerNorm 通常放在残差连接之后，对某层子模块输出\\(x\\)，残差连接为：

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

\\( Sublayer\\)可以是 Self-Attention 或 FFN。






## 大语言模型原理概述

Decoder-Only 就是目前大火的 LLM 的基础架构，目前所有的 LLM 基本都是 Decoder-Only 模型（RWKV、Mamba 等非 Transformer 架构除外）。而引发 LLM 热潮的 ChatGPT，正是 Decoder-Only 系列的代表模型 GPT 系列模型的大成之作。而目前作为开源 LLM 基本架构的 LLaMA 模型，也正是在 GPT 的模型架构基础上优化发展而来。

### GPT
GPT，即 Generative Pre-Training Language Model，是由 OpenAI 团队于 2018年发布的预训练语言模型。

#### 模型架构——Decoder Only
![gpt架构](https://raw.githubusercontent.com/BV003/images/main/llm/3.png)

对于一个自然语言文本的输入，先通过 tokenizer 进行分词并转化为对应词典序号的 input_ids。
输入的 input_ids 首先通过 Embedding 层，再经过 Positional Embedding 进行位置编码。不同于 BERT 选择了可训练的全连接层作为位置编码，GPT 沿用了 Transformer 的经典 Sinusoidal 位置编码，即通过三角函数进行绝对位置编码。

通过 Embedding 层和 Positional Embedding 层编码成 hidden_states 之后，就可以进入到解码器（Decoder），第一代 GPT 模型和原始 Transformer 模型类似，选择了 12层解码器层，但是在解码器层的内部，相较于 Transformer 原始 Decoder 层的双注意力层设计，GPT 的 Decoder 层反而更像 Encoder 层一点。由于不再有 Encoder 的编码输入，Decoder 层仅保留了一个带掩码的注意力层，并且将 LayerNorm 层从 Transformer 的注意力层之后提到了注意力层之前。hidden_states 输入 Decoder 层之后，会先进行 LayerNorm，再进行掩码注意力计算，然后经过残差连接和再一次 LayerNorm 进入到 MLP 中并得到最后输出。

由于不存在 Encoder 的编码结果，Decoder 层中的掩码注意力也是自注意力计算。也就是对一个输入的 hidden_states，会通过三个参数矩阵来生成 query、key 和 value，而不再是像 Transformer 中的 Decoder 那样由 Encoder 输出作为 key 和 value。

#### 预训练任务——CLM

Decoder-Only 的模型结构往往更适合于文本生成任务，因此，Decoder-Only 模型往往选择了最传统也最直接的预训练任务——因果语言模型，Casual Language Model，下简称 CLM。

CLM 可以看作 N-gram 语言模型的一个直接扩展。N-gram 语言模型是基于前 N 个 token 来预测下一个 token，CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token，通过不断重复该过程来实现目标文本序列的生成。也就是说，CLM 是一个经典的补全形式。例如，CLM 的输入和输出可以是：
```
input: 今天天气
output: 今天天气很

input: 今天天气很
output：今天天气很好

```
因此，对于一个输入目标序列长度为 256，期待输出序列长度为 256 的任务，模型会不断根据前 256 个 token、257个 token（输入+预测出来的第一个 token）...... 进行 256 次计算，最后生成一个序列长度为 512 的输出文本，这个输出文本前 256 个 token 为输入，后 256 个 token 就是我们期待的模型输出。CLM 也可以使用海量的自然语言语料进行大规模的预训练。

#### GPT 系列模型的发展
自 GPT-1 推出开始，OpenAI 一直坚信 Decoder-Only 的模型结构和“体量即正义”的优化思路，不断扩大预训练数据集、模型体量并对模型做出一些小的优化和修正，来不断探索更强大的预训练模型。从被 BERT 压制的 GPT-1，到没有引起足够关注的 GPT-2，再到激发了涌现能力、带来大模型时代的 GPT-3，最后带来了跨时代的 ChatGPT，OpenAI 通过数十年的努力证明了其思路的正确性。

下表总结了从 GPT-1 到 GPT-3 的模型结构、预训练语料大小的变化：

| 模型   | Decoder Layer | Hidden_size | 注意力头数 | 注意力维度 | 总参数量 | 预训练语料 |
| ------ | ------------- | ----------- | ---------- | ---------- | -------- | ---------- |
| GPT-1  | 12            | 3072        | 12         | 768        | 0.12B    | 5GB        |
| GPT-2  | 48            | 6400        | 25         | 1600       | 1.5B     | 40GB       |
| GPT-3  | 96            | 49152       | 96         | 12288      | 175B     | 570GB      |


## LLM的训练流程
![训练流程](https://raw.githubusercontent.com/BV003/images/main/llm/4.jpg)

一般而言，训练一个完整的 LLM 需要经过图中的三个阶段——Pretrain、SFT 和 RLHF。

### 预训练
Pretrain，即预训练，是训练 LLM 最核心也是工程量最大的第一步。LLM 的预训练和传统预训练模型非常类似，同样是使用海量无监督文本对随机初始化的模型参数进行训练。预训练任务也都沿承了 GPT 模型的经典预训练任务——因果语言模型（Causal Language Model，CLM）。因果语言模型建模，即和最初的语言模型一致，通过给出上文要求模型预测下一个 token 来进行训练。LLM被喂进大量的文本，比如书、网页、对话、代码等，每次训练时，它看到一段话的前半部分，要去猜接下来的词。标准答案即为原始的语料，训练目标是最大化训练语料的似然，可以通过一系列的损失函数实现。

根据定义，LLM 的核心特点即在于其具有远超传统预训练模型的参数量，同时在更海量的语料上进行预训练。传统预训练模型如 BERT，有 base 和 large 两个版本。BERT-base 模型由 12个 Encoder 层组成，其 hidden_size 为 768，使用 12个头作为多头注意力层，整体参数量为 1亿（110M）；而 BERT-large 模型由 24个 Encoder 层组成，hidden_size 为 1024，有 16个头，整体参数量为 3亿（340M）。同时，BERT 预训练使用了 33亿（3B）token 的语料，在 64块 TPU 上训练了 4天。事实上，相对于传统的深度学习模型，3亿参数量、33亿训练数据的 BERT 已经是一个能力超群、资源消耗巨大的庞然大物。

但是，一般而言的 LLM 通常具有数百亿甚至上千亿参数，即使是广义上最小的 LLM，一般也有十亿（1B）以上的参数量。例如以开山之作 GPT-3 为例，其有 96个 Decoder 层，12288 的 hidden_size 和 96个头，共有 1750亿（175B）参数，比 BERT 大出快 3个数量级。即使是目前流行的小型 LLM 如 Qwen-1.8B，其也有 24个 Decoder 层、2048的 hidden_size 和 16个注意力头，整体参数量达到 18亿（1.8B）。

| 模型        | hidden_layers（隐藏层数） | hidden_size（隐藏层维度） | heads（注意力头数） | 整体参数量 | 预训练数据量 |
|-------------|---------------------------|---------------------------|---------------------|------------|--------------|
| BERT-base   | 12                        | 768                       | 12                  | 0.1B       | 3B           |
| BERT-large  | 24                        | 1024                      | 16                  | 0.3B       | 3B           |
| Qwen-1.8B   | 24                        | 2048                      | 16                  | 1.8B       | 2.2T         |
| LLaMA-7B    | 32                        | 4096                      | 32                  | 7B         | 1T           |
| GPT-3       | 96                        | 12288                     | 96                  | 175B       | 300B         |



## 后训练
找一篇综述
**常见损失函数**  
常见的交叉熵，kl散度，dpo loss

全参数微调 vs. 参数高效微调（LoRA, Prefix Tuning, Adapter）

Instruction Tuning、RLHF
奖励建模 & 强化学习方法（PPO, DPO, GRPO 等）


#### full_sft
#### rlhf
#### reason
#### grpo

## 微调

## 训练工程
数据清洗与 Tokenizer（BPE, SentencePiece）

优化器：AdamW

学习率调度与稳定性技巧
## 推理与部署

模型压缩：量化、蒸馏、剪枝

高效推理：vLLM, TensorRT, ONNX

KV Cache, 批处理优化

## 多模态大模型

### LLAVA

## 应用与实践

下游任务：QA、对话、文本摘要、分类

评测指标：Perplexity, BLEU, ROUGE, BERTScore

对齐与安全：偏见缓解、有害内容检测、系统安全

## 工程与系统

分布式训练：DeepSpeed, FSDP, Megatron-LM, ColossalAI
分布式训练deepspeed的zero系列以及通
高性能计算：CUDA, NCCL 通信

云平台与集群：阿里云/腾讯云/华为云 AI 平台

数据工程：Spark, Flink, 数据标注流程


## 项目与实战
RAG 与Agent

## 各家开源大厂技术报告

qwen、llama网络结构：gqa等，长文本rope，训练细节
dpskv3base，r1的细节




### Article Attribution and License
Author: Michael Liu  
Original Link: [https://bv003.github.io/posts/blog/llm](https://bv003.github.io/posts/blog/llm)  
License: This article is licensed under CC BY-SA 4.0. Redistribution is not permitted without complying with the license requirements.  

### References
1. DatawhaleChina. Happy-LLM: A Tutorial on Large Language Models from Scratch. GitHub.[https://github.com/datawhalechina/happy-llm](https://github.com/datawhalechina/happy-llm)
2. Vaswani, A., Shazeer, N., Parmar, N., et al. "Attention is All You Need." NeurIPS, 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
3. Lilian Weng. Attention? Attention!  [https://lilianweng.github.io/posts/2018-06-24-attention/](https://lilianweng.github.io/posts/2018-06-24-attention/)
