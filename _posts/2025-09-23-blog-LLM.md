---
title: 'Collection of Information on Large Language Models'
date: 2025-09-23
permalink: /posts/pin/blog/llm
pin: true
---

To prepare for an MLsys-related internship, I have organized some relevant resources. I'm sharing them here in the hope that they may help others as well.


<!-- excerpt -->

## 深度学习基础
**Transformer介绍**
为什么 Transformer 会取代 RNN/CNN
Transformer 的核心优势：并行计算、长依赖建模

**Attention 机制**
动机为什么需要 Attention：动态聚合信息
Scaled Dot-Product Attention
手撕实现（numpy版 / PyTorch低级API）

**Multi-Head Attention (MHA)**
为什么需要多头
公式与结构
手撕实现

**Feed Forward Network, FFN**
结构
手撕实现

**Layer Normalization**
为什么需要归一化
对比 BatchNorm 和 LayerNorm
在 Transformer 中的位置：残差连接之后
公式
手撕实现

**Transformer 基础模块拼装**
Encoder Layer
MHA + 残差 + LayerNorm
FFN + 残差 + LayerNorm
Decoder Layer
自注意力
编码器-解码器注意力
FFN
手撕实现
用 PyTorch/numpy 组合前面所有模块


**常见损失函数**  
常见的交叉熵，kl散度，dpo loss

## 预训练模型

BERT、GPT 系列、LLaMA、Qwen

## 训练与微调
全参数微调 vs. 参数高效微调（LoRA, Prefix Tuning, Adapter）

Instruction Tuning、RLHF
奖励建模 & 强化学习方法（PPO, DPO, GRPO 等）

## 训练工程
数据清洗与 Tokenizer（BPE, SentencePiece）

优化器：AdamW

学习率调度与稳定性技巧
## 推理与部署

模型压缩：量化、蒸馏、剪枝

高效推理：vLLM, TensorRT, ONNX

KV Cache, 批处理优化


## 应用与实践

下游任务：QA、对话、RAG、文本摘要、分类

评测指标：Perplexity, BLEU, ROUGE, BERTScore

对齐与安全：偏见缓解、有害内容检测、系统安全

## 工程与系统

分布式训练：DeepSpeed, FSDP, Megatron-LM, ColossalAI
分布式训练deepspeed的zero系列以及通
高性能计算：CUDA, NCCL 通信

云平台与集群：阿里云/腾讯云/华为云 AI 平台

数据工程：Spark, Flink, 数据标注流程


## 项目与实战
RAG 与Agent

## 各家开源大厂技术报告

qwen、llama网络结构：gqa等，长文本rope，训练细节
dpskv3base，r1的细节