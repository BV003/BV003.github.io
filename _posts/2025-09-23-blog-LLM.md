---
title: 'Collection of Information on Large Language Models'
date: 2025-09-23
permalink: /posts/pin/blog/llm
pin: true
---

To prepare for an MLsys-related internship, I have organized some relevant resources. I'm sharing them here in the hope that they may help others as well.


<!-- excerpt -->

## 深度学习基础
Attention 机制
Transformer 架构基础
transformers基础知识：注意力机制，dk纬度，mha（手撕）ffn（手撕），layernorm（手撕）
手撕部分也要好好整理

**常见损失函数**
常见的交叉熵，kl散度，dpo loss

## 预训练模型

BERT、GPT 系列、LLaMA、Qwen

## 训练与微调
全参数微调 vs. 参数高效微调（LoRA, Prefix Tuning, Adapter）

Instruction Tuning、RLHF
奖励建模 & 强化学习方法（PPO, DPO, GRPO 等）

## 训练工程
数据清洗与 Tokenizer（BPE, SentencePiece）

优化器：AdamW

学习率调度与稳定性技巧
## 推理与部署

模型压缩：量化、蒸馏、剪枝

高效推理：vLLM, TensorRT, ONNX

KV Cache, 批处理优化


## 应用与实践

下游任务：QA、对话、RAG、文本摘要、分类

评测指标：Perplexity, BLEU, ROUGE, BERTScore

对齐与安全：偏见缓解、有害内容检测、系统安全

## 工程与系统

分布式训练：DeepSpeed, FSDP, Megatron-LM, ColossalAI
分布式训练deepspeed的zero系列以及通
高性能计算：CUDA, NCCL 通信

云平台与集群：阿里云/腾讯云/华为云 AI 平台

数据工程：Spark, Flink, 数据标注流程


## 项目与实战
RAG 与Agent

## 各家开源大厂技术报告

qwen、llama网络结构：gqa等，长文本rope，训练细节
dpskv3base，r1的细节