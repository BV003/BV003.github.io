---
title: 'Collection of Information on Large Language Models'
date: 2025-09-23
permalink: /posts/blog/llm
---

To prepare for an MLsys-related internship, I have organized some relevant resources. I'm sharing them here in the hope that they may help others as well.


<!-- excerpt -->

## Transformer
#### Transformerä»‹ç» 
Transformer è¯ç”Ÿäº 2017 å¹´ï¼Œç”± Vaswani ç­‰äººåœ¨è®ºæ–‡ã€ŠAttention Is All You Needã€‹ä¸­é¦–æ¬¡æå‡ºã€‚å’Œä¼ ç»Ÿç½‘ç»œRNNï¼ŒCNNç›¸æ¯”ï¼ŒTransformerå…·æœ‰æå¤§çš„ä¼˜åŠ¿ã€‚Transformer é‡‡ç”¨å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰çš„æ¶æ„ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰æœºåˆ¶èƒ½å¤Ÿç›´æ¥æ•æ‰åºåˆ—ä¸­ä»»æ„ä½ç½®çš„ä¾èµ–ã€‚å€ŸåŠ©å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰å’Œä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰Transformerè¿…é€Ÿå–ä»£ RNN å’Œ CNNï¼Œæˆä¸ºç°ä»£ NLP çš„æ ¸å¿ƒåŸºç¡€æ¨¡å‹ã€‚


#### Model Architecture ï¼ˆç¼–ç å™¨å’Œè§£ç å™¨ï¼‰
![æ¨¡å‹æ¶æ„)](https://raw.githubusercontent.com/BV003/images/main/llm/0.png)
![æ¨¡å‹æ¶æ„)](https://raw.githubusercontent.com/BV003/images/main/llm/1.png)

ç¼–ç å™¨ç”±ğ‘=6å±‚ç›¸åŒçš„å±‚å †å è€Œæˆã€‚æ¯ä¸€å±‚åŒ…å«ä¸¤ä¸ªå­å±‚ã€‚ç¬¬ä¸€ä¸ªå­å±‚æ˜¯å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆmulti-head self-attentionï¼‰ï¼Œç¬¬äºŒä¸ªå­å±‚æ˜¯ç®€å•çš„é€ç‚¹å…¨è¿æ¥å‰é¦ˆç½‘ç»œï¼ˆposition-wise fully connected feed-forward networkï¼‰ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªå­å±‚å‘¨å›´ä½¿ç”¨æ®‹å·®è¿æ¥ï¼ˆresidual connectionï¼‰ï¼Œéšåè¿›è¡Œå±‚å½’ä¸€åŒ–ï¼ˆlayer normalizationï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªå­å±‚çš„è¾“å‡ºä¸º

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

è§£ç å™¨ï¼ˆDecoderï¼‰ï¼šè§£ç å™¨åŒæ ·ç”± N=6 å±‚ç›¸åŒçš„å±‚å †å è€Œæˆã€‚é™¤äº†æ¯ä¸ªç¼–ç å™¨å±‚ä¸­çš„ä¸¤ä¸ªå­å±‚ä¹‹å¤–ï¼Œè§£ç å™¨è¿˜æ’å…¥äº†ç¬¬ä¸‰ä¸ªå­å±‚ï¼Œå¯¹ç¼–ç å™¨å †æ ˆçš„è¾“å‡ºè¿›è¡Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—ï¼ˆmulti-head attentionï¼‰ã€‚ä¸ç¼–ç å™¨ç±»ä¼¼ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªå­å±‚å‘¨å›´ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œå¹¶è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚

æ®‹å·®è¿æ¥å°±æ˜¯æŠŠæ¯ä¸ªå­å±‚çš„è¾“å…¥ç›´æ¥åŠ åˆ°å­å±‚è¾“å‡ºä¸Šï¼Œå†ç»è¿‡å½’ä¸€åŒ–ï¼Œè¿™æ ·æ—¢ä¿ç•™äº†åŸå§‹ä¿¡æ¯ï¼Œåˆç¼“è§£äº†æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿è®­ç»ƒæ›´ç¨³å®šã€æ”¶æ•›æ›´å¿«ã€‚

æˆ‘ä»¬è¿˜ä¿®æ”¹äº†è§£ç å™¨å †æ ˆä¸­çš„è‡ªæ³¨æ„åŠ›å­å±‚ï¼Œä½¿å¾—æ¯ä¸ªä½ç½®æ— æ³•å…³æ³¨åç»­ä½ç½®ã€‚è¿™ç§æ©ç ï¼ˆmaskingï¼‰ç»“åˆè¾“å‡ºåµŒå…¥å‘é‡åç§»ä¸€ä¸ªä½ç½®çš„æ–¹å¼ï¼Œç¡®ä¿ä½ç½® \\( i \\) çš„é¢„æµ‹åªèƒ½ä¾èµ–äºä½ç½®å°äº \\( i \\) çš„å·²çŸ¥è¾“å‡ºã€‚

#### Attention æœºåˆ¶
æ³¨æ„åŠ›ï¼ˆattentionï¼‰å‡½æ•°å¯ä»¥è¢«æè¿°ä¸ºï¼šå°†ä¸€ä¸ªæŸ¥è¯¢ï¼ˆqueryï¼‰å’Œä¸€ç»„é”®å€¼å¯¹ï¼ˆkey-value pairsï¼‰æ˜ å°„ä¸ºä¸€ä¸ªè¾“å‡ºï¼Œå…¶ä¸­æŸ¥è¯¢ã€é”®ã€å€¼ä»¥åŠè¾“å‡ºéƒ½æ˜¯å‘é‡ã€‚è¾“å‡ºé€šè¿‡å¯¹å€¼å‘é‡çš„åŠ æƒæ±‚å’Œå¾—åˆ°ï¼Œè€Œæ¯ä¸ªå€¼çš„æƒé‡ç”±æŸ¥è¯¢ä¸å¯¹åº”é”®ä¹‹é—´çš„å…¼å®¹æ€§å‡½æ•°ï¼ˆcompatibility functionï¼‰è®¡ç®—å¾—å‡ºã€‚

æˆ‘ä»¬å¯ä»¥å®šä¹‰ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot-Product Attentionï¼‰ã€‚è¾“å…¥åŒ…æ‹¬ç»´åº¦ä¸º \\( d_k \\) çš„æŸ¥è¯¢ï¼ˆqueriesï¼‰å’Œé”®ï¼ˆkeysï¼‰ï¼Œä»¥åŠç»´åº¦ä¸º \\( d_v \\) çš„å€¼ï¼ˆvaluesï¼‰ã€‚æˆ‘ä»¬å…ˆè®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰é”®çš„ç‚¹ç§¯ï¼Œç„¶åé™¤ä»¥ \\(  \sqrt{d_k} \\)ï¼Œå†é€šè¿‡ softmax å‡½æ•°å¾—åˆ°å„å€¼å‘é‡çš„æƒé‡ï¼ˆSoftmax æ˜¯ä¸€ç§å¸¸ç”¨çš„å½’ä¸€åŒ–å‡½æ•°ï¼‰ã€‚


åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬é€šå¸¸åŒæ—¶å¯¹ä¸€ç»„æŸ¥è¯¢è®¡ç®—æ³¨æ„åŠ›å‡½æ•°ï¼Œå¹¶å°†å…¶æ‰“åŒ…æˆçŸ©é˜µ 
\\(Q\\)ã€‚é”®å’Œå€¼ä¹Ÿåˆ†åˆ«æ‰“åŒ…æˆçŸ©é˜µ \\(K\\) å’Œ \\(V\\)ã€‚è¾“å‡ºçŸ©é˜µçš„è®¡ç®—å…¬å¼ä¸ºï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

æˆ‘ä»¬å¼•å…¥é€šè¿‡ \\(  \sqrt{d_k} \\) çš„ç›®çš„æ˜¯é˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´ softmax çš„æ¢¯åº¦å˜å¾—éå¸¸å°ï¼Œä¿è¯è®­ç»ƒç¨³å®šæ€§ã€‚

```
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    """
    å®ç° Scaled Dot-Product Attention
    å…¬å¼: Attention(Q, K, V) = softmax((Q K^T)/âˆšd_k) V
    """

    def __init__(self, input_dim, dim_k, dim_v):
        """
        Args:
            input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆd_modelï¼‰
            dim_k (int): æŸ¥è¯¢å’Œé”®çš„ç»´åº¦
            dim_v (int): å€¼çš„ç»´åº¦
        """
        super().__init__()
        
        # çº¿æ€§æ˜ å°„ï¼Œå°†è¾“å…¥æ˜ å°„åˆ° Q, K, V
        self.q = nn.Linear(input_dim, dim_k)  # æŸ¥è¯¢
        self.k = nn.Linear(input_dim, dim_k)  # é”®
        self.v = nn.Linear(input_dim, dim_v)  # å€¼
        
        # ç¼©æ”¾å› å­ âˆšd_k
        self.scale = math.sqrt(dim_k)

    def forward(self, x):
        """
        Args:
            x: è¾“å…¥å¼ é‡, ç»´åº¦ (batch_size, seq_len, input_dim)
        Returns:
            output: æ³¨æ„åŠ›è¾“å‡º, ç»´åº¦ (batch_size, seq_len, dim_v)
        """
        # 1. çº¿æ€§æ˜ å°„åˆ° Q, K, V
        # è¾“å‡ºç»´åº¦: (batch_size, seq_len, dim_k) æˆ– (batch_size, seq_len, dim_v)
        Q = self.q(x)
        K = self.k(x)
        V = self.v(x)

        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # ä½¿ç”¨ matmul å¯ä»¥ç›´æ¥å¯¹ 3D å¼ é‡åš batch çŸ©é˜µä¹˜æ³•
        # K.transpose(-2, -1) å°† (batch, seq_len, dim_k) -> (batch, dim_k, seq_len)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (batch, seq_len, seq_len)

        # 3. å¯¹æ³¨æ„åŠ›åˆ†æ•°åš softmaxï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.softmax(scores, dim=-1)  # æ¯è¡Œå½’ä¸€åŒ–

        # 4. æ³¨æ„åŠ›æƒé‡ä¹˜ä»¥ V
        output = torch.matmul(attn_weights, V)  # (batch, seq_len, dim_v)

        return output
```


#### Multi-Head Attention (MHA)

![å¤šå¤´æ³¨æ„åŠ›](https://raw.githubusercontent.com/BV003/images/main/llm/2.png)

æˆ‘ä»¬å‘ç°ï¼Œå°†æŸ¥è¯¢ã€é”®å’Œå€¼åˆ†åˆ«é€šè¿‡ h æ¬¡ä¸åŒçš„ã€å¯å­¦ä¹ çš„çº¿æ€§æ˜ å°„æŠ•å½±åˆ° \\( d_k \\)ã€\\( d_k \\) å’Œ \\( d_v \\) ç»´åº¦ä¸Šæ›´ä¸ºæœ‰æ•ˆã€‚åœ¨è¿™äº›æŠ•å½±åçš„æŸ¥è¯¢ã€é”®å’Œå€¼ä¸Šï¼Œæˆ‘ä»¬å¹¶è¡Œæ‰§è¡Œæ³¨æ„åŠ›å‡½æ•°ï¼Œå¾—åˆ° \\( d_v \\) ç»´åº¦çš„è¾“å‡ºå€¼ã€‚

å¤šå¤´æ³¨æ„åŠ›å…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨æ¥è‡ªä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¸åŒä½ç½®çš„ä¿¡æ¯ã€‚è€Œå•å¤´æ³¨æ„åŠ›åœ¨å¹³å‡æ“ä½œä¸‹ä¼šæŠ‘åˆ¶è¿™ä¸€èƒ½åŠ›ã€‚

å…¬å¼è¡¨ç¤ºä¸ºï¼š

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

å…¶ä¸­ï¼š

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

æŠ•å½±çŸ©é˜µä¸ºå‚æ•°çŸ©é˜µï¼š

$$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$$

$$W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$$

$$W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$$

$$W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}}$$


åœ¨æœ¬å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ h = 8 ä¸ªå¹¶è¡Œæ³¨æ„åŠ›å±‚ï¼ˆå³å¤´ï¼‰ã€‚å¯¹äºæ¯ä¸ªå¤´ï¼Œæˆ‘ä»¬è®¾ç½® \\( d_k \\) = \\( d_v \\) =\\( d_{\text{model}} \\) /h = 64ã€‚ç”±äºæ¯ä¸ªå¤´çš„ç»´åº¦è¾ƒå°ï¼Œæ€»è®¡ç®—æˆæœ¬ä¸ä½¿ç”¨å…¨ç»´åº¦çš„å•å¤´æ³¨æ„åŠ›ç›¸å½“ã€‚

```
import torch
import torch.nn as nn
import math

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, input_dim, num_heads, dim_k, dim_v):
        """
        Multi-Head Self-Attention
        
        Args:
            input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆd_modelï¼‰
            num_heads (int): æ³¨æ„åŠ›å¤´æ•°é‡ï¼ˆhï¼‰
            dim_k (int): æ‰€æœ‰å¤´çš„æ€»æŸ¥è¯¢/é”®ç»´åº¦ï¼ˆé€šå¸¸ = input_dimï¼‰
            dim_v (int): æ‰€æœ‰å¤´çš„æ€»å€¼ç»´åº¦ï¼ˆé€šå¸¸ = input_dimï¼‰
        """
        super().__init__()
        assert dim_k % num_heads == 0, "dim_k must be divisible by num_heads"
        assert dim_v % num_heads == 0, "dim_v must be divisible by num_heads"

        self.num_heads = num_heads
        self.dim_per_head_k = dim_k // num_heads
        self.dim_per_head_v = dim_v // num_heads

        # Qã€Kã€V çš„çº¿æ€§æ˜ å°„å±‚
        self.q_linear = nn.Linear(input_dim, dim_k)
        self.k_linear = nn.Linear(input_dim, dim_k)
        self.v_linear = nn.Linear(input_dim, dim_v)

        # è¾“å‡ºçº¿æ€§æ˜ å°„ï¼ŒæŠŠæ‹¼æ¥åçš„å¤šå¤´è¾“å‡ºæ˜ å°„å› input_dim
        self.out_linear = nn.Linear(dim_v, input_dim)

        # ç¼©æ”¾å› å­ âˆšd_k
        self.scale = math.sqrt(self.dim_per_head_k)

    def forward(self, x):
        """
        x: (batch_size, seq_len, input_dim)
        """
        b, s, _ = x.size()

        # 1. çº¿æ€§æ˜ å°„åˆ° Q, K, V
        # è¾“å‡ºç»´åº¦: (b, s, num_heads * dim_per_head)
        Q = self.q_linear(x)
        K = self.k_linear(x)
        V = self.v_linear(x)

        # 2. reshape å¹¶è½¬ç½®ä¸ºå¤šå¤´: (b, num_heads, s, dim_per_head)
        # viewæ‹†åˆ†æœ€åä¸€ä½
        Q = Q.view(b, s, self.num_heads, self.dim_per_head_k).transpose(1, 2)
        K = K.view(b, s, self.num_heads, self.dim_per_head_k).transpose(1, 2)
        V = V.view(b, s, self.num_heads, self.dim_per_head_v).transpose(1, 2)

        # 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # Q * K^T / sqrt(d_k)
        # Q: (b, h, s, d_k), K: (b, h, s, d_k) -> (b, h, s, s)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale

        # 4. softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.softmax(scores, dim=-1)

        # 5. æƒé‡ä¹˜ä»¥ V å¾—åˆ°æ¯ä¸ªå¤´çš„è¾“å‡º: (b, h, s, dim_per_head_v)
        attn_output = torch.matmul(attn_weights, V)

        # 6. è½¬ç½®å¹¶æ‹¼æ¥å¤šå¤´è¾“å‡º: (b, s, num_heads * dim_per_head_v)
        attn_output = attn_output.transpose(1, 2).contiguous().view(b, s, -1)

        # 7. è¾“å‡ºçº¿æ€§æ˜ å°„å› input_dim: (b, s, input_dim)
        out = self.out_linear(attn_output)

        return out

```
Masked ç‰ˆæœ¬åœ¨ softmax å‰å¢åŠ ä¸€ä¸ª maskï¼š

$$
\text{MaskedAttention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} + M \right) V
$$

å…¶ä¸­ $$M \in \mathbb{R}^{\text{seq_len} \times \text{seq_len}}$$ æ˜¯ä¸Šä¸‰è§’æ©ç çŸ©é˜µ

æ©ç å…ƒç´ ï¼š

$$
M_{i,j} =
\begin{cases}
0, & j \leq i \\
-\infty, & j > i
\end{cases}
$$

è¿™æ · softmax åï¼Œæœªæ¥ token çš„æ³¨æ„åŠ›æƒé‡è¢« ç½®ä¸º 0ï¼Œæ¨¡å‹æ— æ³•è®¿é—®æœªæ¥ä¿¡æ¯ã€‚

#### Feed-Forward Network, FFN

é™¤äº†æ³¨æ„åŠ›å­å±‚ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„ç¼–ç å™¨å’Œè§£ç å™¨ä¸­çš„æ¯ä¸€å±‚è¿˜åŒ…å«ä¸€ä¸ª å…¨è¿æ¥å‰é¦ˆç½‘ç»œï¼Œå®ƒä¼šåˆ†åˆ«å¹¶ä¸”ä¸€è‡´åœ°åº”ç”¨äºåºåˆ—ä¸­çš„æ¯ä¸€ä¸ªä½ç½®ã€‚  
è¿™ä¸ªç½‘ç»œç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸­é—´çš„ä¸€ä¸ª ReLU æ¿€æ´»å‡½æ•°ç»„æˆï¼š

$$
\text{FFN}(x) = \max\left(0, xW_1 + b_1\right)W_2 + b_2
$$

å°½ç®¡è¿™äº›çº¿æ€§å˜æ¢åœ¨ä¸åŒä½ç½®ä¸Šæ˜¯ç›¸åŒçš„ï¼Œä½†åœ¨ä¸åŒå±‚ä¹‹é—´å®ƒä»¬çš„å‚æ•°æ˜¯ä¸åŒçš„ã€‚
è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ä¸º\\( d_{\text{model}} \\)=512ï¼Œè€Œå†…éƒ¨å±‚çš„ç»´åº¦ä¸º\\( d_{\text{ff}} \\)=2048ã€‚

```
import torch
import torch.nn as nn

class FFN(nn.Module):
    def __init__(self, d_model=512, d_ff=2048, dropout=0.0):
        super().__init__()
        # ç­‰ä»·äºä¸¤ä¸ªçº¿æ€§å±‚å’Œä¸­é—´çš„ ReLU
        self.w1 = nn.Linear(d_model, d_ff)   # x @ W1^T + b1  ï¼ˆPyTorch Linear è‡ªå¸¦åç½®ï¼‰
        self.w2 = nn.Linear(d_ff, d_model)   # hidden @ W2^T + b2
        self.relu = nn.ReLU()
        # dropoutä¸ºä¸¢å¼ƒç¥ç»å…ƒçš„æ¦‚ç‡
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        hidden = self.w1(x)       # (batch, seq, d_ff)
        hidden = self.relu(hidden)
        hidden = self.dropout(hidden)
        out = self.w2(hidden)     # (batch, seq_len, d_model)
        return out
```

#### Embeddings and Softmax
æˆ‘ä»¬ä½¿ç”¨ å¯å­¦ä¹ çš„åµŒå…¥ï¼ˆlearned embeddingsï¼‰ å°†è¾“å…¥å’Œè¾“å‡ºçš„ token è½¬æ¢ä¸ºç»´åº¦ä¸º \\( d_{\text{model}} \\)çš„å‘é‡ã€‚  
åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿä½¿ç”¨é€šå¸¸çš„å¯å­¦ä¹ çº¿æ€§å˜æ¢ï¼ˆlinear transformationï¼‰å’Œsoftmaxå‡½æ•°ï¼Œå°†è§£ç å™¨çš„è¾“å‡ºè½¬æ¢ä¸ºé¢„æµ‹çš„ä¸‹ä¸€ä¸ª tokençš„æ¦‚ç‡åˆ†å¸ƒã€‚  
åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œä¸¤ä¸ªåµŒå…¥å±‚ï¼ˆè¾“å…¥åµŒå…¥å’Œè¾“å‡ºåµŒå…¥ï¼‰ä»¥åŠ softmax å‰çš„çº¿æ€§å˜æ¢å…±äº«ç›¸åŒçš„æƒé‡çŸ©é˜µã€‚
åœ¨åµŒå…¥å±‚ä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šå°†è¿™äº›æƒé‡ä¹˜ä»¥ \\( \sqrt{d_{\text{model}}} \\)è¿›è¡Œç¼©æ”¾ã€‚

#### Positional Encoding
æˆ‘ä»¬åœ¨ç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆåº•éƒ¨çš„è¾“å…¥åµŒå…¥ä¸­ï¼ŒåŠ å…¥äº†ä½ç½®ç¼–ç ï¼ˆpositional encodingsï¼‰ã€‚ä½ç½®ç¼–ç çš„ç»´åº¦ä¸åµŒå…¥ç»´åº¦ç›¸åŒ\\( d_{\text{model}} \\)ï¼Œä»¥ä¾¿å¯ä»¥ç›´æ¥ä¸åµŒå…¥å‘é‡ç›¸åŠ ã€‚

$$
\begin{align*}
\text{PE}(pos, 2i) &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
\text{PE}(pos, 2i + 1) &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{align*}
$$

å…¶ä¸­ \\( pos\\)è¡¨ç¤ºä½ç½®ï¼Œ\\(i\\)è¡¨ç¤ºç»´åº¦ç´¢å¼•ã€‚æ¢å¥è¯è¯´ï¼Œæ¯ä¸€ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªæ­£å¼¦æˆ–ä½™å¼¦å‡½æ•°ã€‚

#### Layer Normalization
å½’ä¸€åŒ–çš„ç›®çš„ä¸»è¦æ˜¯åŠ é€Ÿè®­ç»ƒã€ç¨³å®šæ¢¯åº¦ã€æ”¹å–„æ”¶æ•›æ€§ï¼Œç¼“è§£è¶…å‚æ•°æ•æ„Ÿæ€§ã€‚LayerNorm é€šå¸¸æ”¾åœ¨æ®‹å·®è¿æ¥ä¹‹åï¼Œå¯¹æŸå±‚å­æ¨¡å—è¾“å‡º\\(x\\)ï¼Œæ®‹å·®è¿æ¥ä¸ºï¼š

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

\\( Sublayer\\)å¯ä»¥æ˜¯ Self-Attention æˆ– FFNã€‚






## å¤§è¯­è¨€æ¨¡å‹åŸç†æ¦‚è¿°





## é¢„è®­ç»ƒ

## åè®­ç»ƒ
æ‰¾ä¸€ç¯‡ç»¼è¿°
**å¸¸è§æŸå¤±å‡½æ•°**  
å¸¸è§çš„äº¤å‰ç†µï¼Œklæ•£åº¦ï¼Œdpo loss

å…¨å‚æ•°å¾®è°ƒ vs. å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRA, Prefix Tuning, Adapterï¼‰

Instruction Tuningã€RLHF
å¥–åŠ±å»ºæ¨¡ & å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆPPO, DPO, GRPO ç­‰ï¼‰


## å¾®è°ƒ

## è®­ç»ƒå·¥ç¨‹
æ•°æ®æ¸…æ´—ä¸ Tokenizerï¼ˆBPE, SentencePieceï¼‰

ä¼˜åŒ–å™¨ï¼šAdamW

å­¦ä¹ ç‡è°ƒåº¦ä¸ç¨³å®šæ€§æŠ€å·§
## æ¨ç†ä¸éƒ¨ç½²

æ¨¡å‹å‹ç¼©ï¼šé‡åŒ–ã€è’¸é¦ã€å‰ªæ

é«˜æ•ˆæ¨ç†ï¼švLLM, TensorRT, ONNX

KV Cache, æ‰¹å¤„ç†ä¼˜åŒ–

## å¤šæ¨¡æ€å¤§æ¨¡å‹

### LLAVA

## åº”ç”¨ä¸å®è·µ

ä¸‹æ¸¸ä»»åŠ¡ï¼šQAã€å¯¹è¯ã€æ–‡æœ¬æ‘˜è¦ã€åˆ†ç±»

è¯„æµ‹æŒ‡æ ‡ï¼šPerplexity, BLEU, ROUGE, BERTScore

å¯¹é½ä¸å®‰å…¨ï¼šåè§ç¼“è§£ã€æœ‰å®³å†…å®¹æ£€æµ‹ã€ç³»ç»Ÿå®‰å…¨

## å·¥ç¨‹ä¸ç³»ç»Ÿ

åˆ†å¸ƒå¼è®­ç»ƒï¼šDeepSpeed, FSDP, Megatron-LM, ColossalAI
åˆ†å¸ƒå¼è®­ç»ƒdeepspeedçš„zeroç³»åˆ—ä»¥åŠé€š
é«˜æ€§èƒ½è®¡ç®—ï¼šCUDA, NCCL é€šä¿¡

äº‘å¹³å°ä¸é›†ç¾¤ï¼šé˜¿é‡Œäº‘/è…¾è®¯äº‘/åä¸ºäº‘ AI å¹³å°

æ•°æ®å·¥ç¨‹ï¼šSpark, Flink, æ•°æ®æ ‡æ³¨æµç¨‹


## é¡¹ç›®ä¸å®æˆ˜
RAG ä¸Agent

## å„å®¶å¼€æºå¤§å‚æŠ€æœ¯æŠ¥å‘Š

qwenã€llamaç½‘ç»œç»“æ„ï¼šgqaç­‰ï¼Œé•¿æ–‡æœ¬ropeï¼Œè®­ç»ƒç»†èŠ‚
dpskv3baseï¼Œr1çš„ç»†èŠ‚




### Article Attribution and License
Author: Michael Liu  
Original Link: https://bv003.github.io/posts/blog/llm  
License: This article is licensed under CC BY-SA 4.0. Redistribution is not permitted without complying with the license requirements.  

### References
[1] DatawhaleChina. Happy-LLM: A Tutorial on Large Language Models from Scratch. GitHub.[https://github.com/datawhalechina/happy-llm](https://github.com/datawhalechina/happy-llm)
[2] Vaswani, A., Shazeer, N., Parmar, N., et al. "Attention is All You Need." *NeurIPS*, 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
