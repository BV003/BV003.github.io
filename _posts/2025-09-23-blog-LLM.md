---
title: 'Collection of Information on Large Language Models'
date: 2025-09-23
permalink: /posts/pin/blog/llm
pin: true
---

To prepare for an MLsys-related internship, I have organized some relevant resources. I'm sharing them here in the hope that they may help others as well.


<!-- excerpt -->

## 深度学习基础
#### Transformer介绍 
Transformer 诞生于 2017 年，由 Vaswani 等人在论文《Attention Is All You Need》中首次提出。和传统网络RNN，CNN相比，Transformer具有极大的优势。Transformer 采用完全基于注意力机制（Attention）的架构，通过自注意力（Self-Attention）机制能够直接捕捉序列中任意位置的依赖。借助多头注意力（Multi-Head Attention）和位置编码（Positional Encoding）Transformer迅速取代 RNN 和 CNN，成为现代 NLP 的核心基础模型。


#### Model Architecture （编码器和解码器）
![!(模型架构)(photo.jpg)](https://private-user-images.githubusercontent.com/161697323/493679675-47c92fd2-b5cf-44c4-bc2d-178845b01ec5.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTg3ODQwOTksIm5iZiI6MTc1ODc4Mzc5OSwicGF0aCI6Ii8xNjE2OTczMjMvNDkzNjc5Njc1LTQ3YzkyZmQyLWI1Y2YtNDRjNC1iYzJkLTE3ODg0NWIwMWVjNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwOTI1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDkyNVQwNzAzMTlaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01YmUwMWMzMGU3ZTU0YTE1YThmN2YyNTA1NTMyY2RhNjc4ZTAwZDA5MWQzZmZmYjdkOWJkNjk5OTc2NzQxN2UyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.R8l_jYw-1hu9ajZGmemdh7VbBnMXAY8s4WcOvb1o7GU)

编码器由𝑁=6层相同的层堆叠而成。每一层包含两个子层。第一个子层是多头自注意力机制（multi-head self-attention），第二个子层是简单的逐点全连接前馈网络（position-wise fully connected feed-forward network）。我们在每个子层周围使用残差连接（residual connection），随后进行层归一化（layer normalization）。也就是说，每个子层的输出为

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

解码器（Decoder）：解码器同样由 N=6 层相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器还插入了第三个子层，对编码器堆栈的输出进行多头注意力计算（multi-head attention）。与编码器类似，我们在每个子层周围使用残差连接，并进行层归一化。

我们还修改了解码器堆栈中的自注意力子层，使得每个位置无法关注后续位置。这种掩码（masking）结合输出嵌入向量偏移一个位置的方式，确保位置 \\( i \\) 的预测只能依赖于位置小于 $i$ 的已知输出。

#### Attention 机制

手撕实现（PyTorch低级API）

注意力（attention）函数可以被描述为：将一个查询（query）和一组键值对（key-value pairs）映射为一个输出，其中查询、键、值以及输出都是向量。输出通过对值向量的加权求和得到，而每个值的权重由查询与对应键之间的兼容性函数（compatibility function）计算得出。

我们可以定义缩放点积注意力（Scaled Dot-Product Attention）。输入包括维度为 $d_k$ 的查询（queries）和键（keys），以及维度为 $d_v$ 的值（values）。我们先计算查询与所有键的点积，然后除以 $ \sqrt{d_k} $，再通过 softmax 函数得到各值向量的权重。


在实际操作中，我们通常同时对一组查询计算注意力函数，并将其打包成矩阵 
$Q$。键和值也分别打包成矩阵 $K$ 和 $V$。输出矩阵的计算公式为：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

我们引入通过 $ \sqrt{d_k} $ 的目的是防止点积过大导致 softmax 的梯度变得非常小，保证训练稳定性。

**Multi-Head Attention (MHA)**
为什么需要多头
公式与结构
手撕实现

**Feed Forward Network, FFN**
结构
手撕实现

**Layer Normalization**
为什么需要归一化
对比 BatchNorm 和 LayerNorm
在 Transformer 中的位置：残差连接之后
公式
手撕实现

**Transformer 基础模块拼装**
Encoder Layer
MHA + 残差 + LayerNorm
FFN + 残差 + LayerNorm
Decoder Layer
自注意力
编码器-解码器注意力
FFN
手撕实现
用 PyTorch/numpy 组合前面所有模块


**常见损失函数**  
常见的交叉熵，kl散度，dpo loss

## 预训练模型

BERT、GPT 系列、LLaMA、Qwen

## 训练与微调
全参数微调 vs. 参数高效微调（LoRA, Prefix Tuning, Adapter）

Instruction Tuning、RLHF
奖励建模 & 强化学习方法（PPO, DPO, GRPO 等）

## 训练工程
数据清洗与 Tokenizer（BPE, SentencePiece）

优化器：AdamW

学习率调度与稳定性技巧
## 推理与部署

模型压缩：量化、蒸馏、剪枝

高效推理：vLLM, TensorRT, ONNX

KV Cache, 批处理优化


## 应用与实践

下游任务：QA、对话、RAG、文本摘要、分类

评测指标：Perplexity, BLEU, ROUGE, BERTScore

对齐与安全：偏见缓解、有害内容检测、系统安全

## 工程与系统

分布式训练：DeepSpeed, FSDP, Megatron-LM, ColossalAI
分布式训练deepspeed的zero系列以及通
高性能计算：CUDA, NCCL 通信

云平台与集群：阿里云/腾讯云/华为云 AI 平台

数据工程：Spark, Flink, 数据标注流程


## 项目与实战
RAG 与Agent

## 各家开源大厂技术报告

qwen、llama网络结构：gqa等，长文本rope，训练细节
dpskv3base，r1的细节