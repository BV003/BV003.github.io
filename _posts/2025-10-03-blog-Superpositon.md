---
title: 'Superposition in Neural Networks: Tackling the Challenge of Polysemantic Neurons'
date: 2025-10-03
permalink: /posts/blog/superposition
---

This blog summarizes the superposition problem, with the aim of addressing the issue that a single neuron or attention head may simultaneously represent multiple concepts.


<!-- excerpt -->

## Problem Description
In OpenAI’s blog post *“Language models can explain neurons in language models”*, researchers found through analysis of models such as GPT-2 XL that most neurons exhibit significant polysemanticity. That is, a single neuron does not activate solely for one type of semantic pattern (e.g., “Marvel movie-related words” or “past tense verbs”), but instead responds to multiple unrelated or weakly related semantic concepts.

Existing interpretability methods (such as natural language explanations generated by GPT-4) struggle to capture this complexity. When a neuron corresponds to multiple concepts, the explanation often only covers one or a subset of them, leading to incomplete explanations. To accommodate multiple meanings, the explanation might become overly broad—for instance, summarizing “activates for both ‘not all’ and ‘partial negation phrases’” simply as “activates for negation-related terms”—thereby reducing interpretive precision.

Regarding the causes of polysemanticity, researchers offered several perspectives.
First is the superposition effect. It is hypothesized that polysemanticity arises from the model’s strategy of superimposed representations, where multiple semantic features are compressed into the same group of neurons rather than being assigned to distinct ones. This mechanism is more prominent in parameter-limited models: to encode sufficient semantic information with a limited number of neurons, the model is forced to let individual neurons play multiple semantic roles. In deeper models (such as the GPT-3 family), superposition becomes even more pronounced, resulting in a higher proportion of polysemantic neurons.

Second, experiments show that models using sparse activation functions (e.g., ReLU, or configurations with low activation density such as 0.01) exhibit lower degrees of polysemanticity than those using dense activation functions like GELU. The reason is that sparse activations force neurons to “focus on fewer patterns,” reducing the likelihood that a single neuron responds to multiple semantic concepts.

Moreover, increasing model scale exacerbates polysemanticity. Analysis of the GPT-3 series (ranging from 98K to 6.7B parameters) shows that the proportion of polysemantic neurons grows with model size, while interpretability scores (correlation coefficients) decline. In deeper layers especially, polysemanticity leads to blurred semantic boundaries for individual neurons.

In the same blog post, OpenAI researchers also proposed several approaches to mitigate polysemanticity. One method, called “finding explainable directions,” reduces the impact of polysemanticity by optimizing linear combinations of neurons rather than interpreting single neurons. Specifically, a random unit vector θ is initialized, and coordinate ascent is used to iteratively optimize it—first by finding natural language descriptions that best explain its activations, then adjusting θ based on explanation scores—ultimately yielding a “virtual neuron” with lower polysemanticity.

The researchers further suggest that future work could explore non-negative matrix factorization (NMF) or singular value decomposition (SVD) to decompose polysemantic activations into multiple independent monosemantic components, and then generate explanations for each component. In essence, this approach aims to break down the polysemanticity problem into a series of monosemantic subproblems.




## Towards Monosemanticity: Decomposing Language Models With Dictionary Learning

This is an Anthropic blog post, whose central argument is that features obtained through dictionary learning are more monosemantic than individual neurons.

In neural networks, the most natural computational unit—the neuron itself—is not necessarily a natural unit for human understanding. This is because many neurons exhibit polysemanticity: they respond to multiple seemingly unrelated inputs. Such polysemanticity makes it difficult for researchers to reason about the behavior of the entire network by analyzing individual neurons alone.

In a previous paper, *Toy Models of Superposition*, the authors proposed three strategies for identifying a set of sparse and interpretable feature representations when features are obscured by superposition:
(1) constructing models without superposition, for instance by encouraging sparse activations;
(2) using dictionary learning to identify an overcomplete feature basis in models that do exhibit superposition; and
(3) hybrid approaches that combine the two.
Since that publication, the authors have explored all three methods. Ultimately, they presented several counterexamples leading them to conclude that simply relying on sparse architectures (method 1) is insufficient to prevent polysemanticity, while standard dictionary learning methods (method 2) suffer from severe overfitting issues.

In this article, the authors employ a weak form of dictionary learning—specifically, a sparse autoencoder—to extract learned features from a trained model. Compared with neurons in the model, these learned features serve as more monosemantic units of analysis. The goal of the paper is to demonstrate in detail how sparse autoencoders achieve convincing results on two key objectives:
(1) extracting interpretable features from superposition, and
(2) enabling foundational interpretability analyses of the circuits within neural networks.

More concretely, the authors take a single-layer Transformer model whose MLP layer contains 512 neurons, and train a sparse autoencoder on 8 billion data points to decompose the MLP activations into relatively interpretable features. The expansion factor ranges from 1× (512 features) to 256× (131,072 features). The main analysis focuses on a specific experiment that learned 4,096 features, referred to as A/1.

### Experimental Results

The sparse autoencoder is capable of extracting relatively monosemantic features. The authors provide evidence from four perspectives:
(1) detailed case studies of a few features that activate in specific contexts, along with corresponding computational proxies;
(2) manual analyses of a large random sample of features;
(3) automated interpretability analyses of all feature activations learned by the autoencoder; and
(4) automated interpretability analyses of all feature logit weights.
Across the latter three analyses, most learned features were found to be interpretable. The authors do not claim that their explanations fully capture all aspects of feature behavior, but by constructing consistent metrics of interpretability for both features and neurons, they quantitatively demonstrate the relative interpretability advantage of features.

The sparse autoencoder can also reveal interpretable features that are “invisible” in the neuron basis. For example, the authors identified features representing the Hebrew alphabet that did not appear in any neuron’s top activation examples but were effectively recognized by the autoencoder. Moreover, features learned by the sparse autoencoder can be used to intervene in and steer Transformer generation behavior. For instance, activating the “Base64 feature” causes the model to generate Base64-encoded text, while activating the “Arabic script feature” leads the model to produce Arabic text.

The learned features also exhibit a degree of universality. When applying the sparse autoencoder to different Transformer language models, the resulting features are largely similar—and in fact, more similar to each other than to the neurons of their respective original models.

As the size of the autoencoder increases, features undergo a process of feature splitting. When the authors gradually expand the width of the autoencoder from 512 (equal to the number of neurons) to over 131,000 (a 256× expansion), they observe that some features naturally divide into “families.” For example, a single Base64 feature present in a smaller dictionary splits into three finer—but still interpretable—features in a larger dictionary. Thus, autoencoders of different scales offer different “resolutions” for understanding the same underlying phenomena.

Even with only 512 neurons, the model can represent tens of thousands of features. Despite the small size of the MLP layer, expanding the sparse autoencoder continually reveals new features. These features can form systems resembling finite-state automata, enabling complex behaviors. For instance, the authors found that certain features can work together to generate valid HTML structures.

### Experimental Setup

The authors select a single-layer Transformer + MLP (ReLU) as their research target, calling it “the simplest language model we don’t yet understand.” Their aim is to determine whether the activations of this MLP layer can be explained in terms of more fundamental, interpretable units—features—rather than the coarse unit of neurons.

They propose using a sparse autoencoder to decompose MLP activations, emphasizing one key design choice: the number of decomposed features is much greater than the number of neurons (an overcomplete decomposition). This choice reflects their belief that the MLP layer compresses many more semantic features through superposition.

We may adopt the following perspective: the model’s activations can be expressed as a linear combination of multiple “feature directions.”


$$ x_j \approx b + \sum_{i} f_i(x_j) d_i $$

The activation vector can be represented as a combination of multiple feature vectors. We use a sparse autoencoder to achieve this decomposition: the encoder computes the feature activations \\( f_i(x) \\), while the decoder’s column vectors represent each feature’s direction \\( d_{i} \\). If we can reconstruct the activations using sparse features, we can uncover the semantic units that have been “superimposed”, thereby understanding how the model manages to store many concepts within a small number of neurons. Mathematically, this formulation is identical to the classical dictionary learning problem.

Our sparse autoencoder consists of:

* an input layer with a bias term,
* a linear encoder layer with bias and ReLU activation, and
* a linear decoder layer with bias.

In toy models, we found that bias terms are crucial for the performance of the autoencoder. The model is trained using the Adam optimizer to reconstruct the MLP layer activations from the Transformer model. The loss function is composed of mean squared error (MSE) plus an L1 regularization term to encourage sparsity.

During training, we identified two key principles:

1. Data scale matters enormously. The success of the sparse autoencoder depends heavily on the size and diversity of the dataset used for training.
2. Neuron death can occur during training. Some neurons in the autoencoder may “die”—that is, they fail to activate across most data points.

To assess the validity of the autoencoder, we employ several evaluation methods, including manual inspection, feature density measurement, reconstruction loss analysis, and toy model validation. These methods together help determine whether the learned sparse autoencoder effectively captures meaningful and interpretable feature representations.


## POLYSEMANTIC INTERFERENCE  TRANSFERS AND PREDICTS CROSS-MODEL INFLUENCE

By leveraging Sparse Autoencoders (SAEs), the authors mapped the polysemantic topology of two small models (Pythia-70M and GPT-2-Small) to identify pairs of SAE features that are semantically unrelated yet interfere with each other inside the model. The authors conducted interventions at four levels—prompt, word, feature, and neuron—and measured the resulting changes in the next-token prediction distribution, thereby revealing the polysemantic structures that expose the systematic vulnerabilities of these models.

The main research findings of the paper are as follows.

Experimental evidence shows that interventions leveraging the polysemantic structures of large language models (LLMs) can effectively manipulate model outputs. Specifically, by targeting features and tokens (using the steering vector technique) as well as prompts (through prompt injection), we can reliably induce the model to express the desired semantics—even when such interventions are misaligned or interfering with the target semantics.

We found polysemantic structures that persist across models. By collecting shared interfering features from Pythia-70M and GPT-2-Small and applying them to steer Llama-3.1-8B/70B-Instruct and Gemma-2-9B-Instruct, we still observed significant intervention effects, indicating a semantically consistent architecture that extends beyond specific model implementations.

We investigated interference patterns that appear counterintuitive yet remain stable across models. Post hoc annotation of higher-level semantic relations explains only a small subset of cases, suggesting that the robust regularities learned by models are largely invisible to human interpretation.

At the neuron level, intervention analyses show that highly polysemantic neurons are more fragile: modifying their activations leads to greater semantic changes in model outputs. For “super-neurons” (those activated by more than 500 features), amplifying their activations strongly alters model behavior, while suppressing them has a much weaker effect. This suggests that they may serve as key nodes in the semantic architecture.

Sparse autoencoders can disentangle polysemantic neurons into monosemantic sparse features, but the semantic decomposition hierarchy of these features is not always consistent. For example, a neuron related to the concept of “dog” may be decomposed into features representing different dog breeds; whereas another neuron encoding both the concepts “cat” and “car” may be decomposed into separate features representing “cat” and “car.” This leads to variations in the granularity of the resulting monosemantic features. To mitigate this inconsistency in semantic hierarchy, we employ agglomerative clustering to align feature representations so that all features reach a unified semantic level. This serves two purposes: (1) to quantify neuron polysemanticity, and (2) to separate feature groups with low surface-level semantic similarity, thereby facilitating subsequent analyses.

To identify higher-level and semantically more distinct features, we utilize automatically generated feature descriptions and their embeddings to compute the semantic similarity between pairs of SAE features. Prior studies have proposed several heuristic approaches for identifying semantically independent SAE features: some suggest using 0.5 as the threshold to distinguish semantically distinct feature clusters; others show that cosine similarities between unrelated concepts typically fall in the range of 0.05–0.30. Building on these insights, we perform agglomerative clustering at each SAE layer, using four progressively stricter similarity thresholds (0.40, 0.30, 0.20, and 0.15). This design allows us to assess the robustness of experimental results under varying semantic distinctness thresholds, while also ensuring sufficient feature density to support subsequent experiments.

Starting from Pythia-70M and GPT-2-Small, we conducted polysemantic interventions using three complementary methods: feature direction steering, token gradient steering, and prompt injection.


### Article Attribution and License
Author: Michael Liu  
Original Link: [https://bv003.github.io/posts/blog/superposition](https://bv003.github.io/posts/blog/superposition)  
License: This article is licensed under CC BY-SA 4.0. Redistribution is not permitted without complying with the license requirements.  

### References
1. Language models can explain neurons in language models. 2023. [link](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)
2. Circuit Tracing: Revealing Computational Graphs in Language Models. 2025. [link](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)
3. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning.[link](https://transformer-circuits.pub/2023/monosemantic-features/index.html)
4. Signal in the noise: Polysemantic interference transfers and predicts cross-model influence. [link](https://arxiv.org/html/2505.11611v2)