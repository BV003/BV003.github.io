---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Research Assistant.

> **Announcement:** I am looking for **Please fill out !**

## Research

My research is dedicated to developing more **capable, efficient, and aligned** Large Language Models (LLMs). I work across the entire LLM lifecycle, including training paradigms, data and inference efficiency, and the foundations of representations.

### Post-Training: Aligning and Enhancing LLMs
My recent work designs better post-training algorithms to improve reasoning, factuality, preference alignment, and model-based evaluation.

### Efficiency: Overcoming Data and Inference Bottlenecks
My research addresses critical bottlenecks in data efficiency and inference efficiency, from synthetic data generation to faster decoding.

### Foundations of Representation Learning
My work investigates the core principles of representation learning, uncovers limitations in language model representations, and proposes novel pre-training objectives to build more robust and capable foundation models.

## News

- **2025.05** â€” Two papers !

## Education

- **B.S. (2022)** Computer Engineering, WH University  

## Contact

- **Email:**  
- **Office:**  
